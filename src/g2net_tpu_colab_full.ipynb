{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"g2net_tpu_colab_full.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RQMzadIy799t"},"source":["\n","# G2Net Playground TPU\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pvF9QKLm_cEJ"},"source":["This notebook has been created to serve as a guided tour to training G2Net models by using Colab TPU. The code **IS BASED** on the project-like tree uploaded to my GitHub as well, a code that rocketed the team (ROC & Roll) to top 8% and a bronze medal in its first competition. With all that said, let's ROC it! (Hope you get the joke)"]},{"cell_type":"markdown","metadata":{"id":"m6VkgQFwBIsw"},"source":["## 1. Introduction"]},{"cell_type":"markdown","metadata":{"id":"VcftmfyrhFo9"},"source":["### 1.1 Problem Description"]},{"cell_type":"markdown","metadata":{"id":"507TSYzKhF_c"},"source":["Not only a network of Gravitational Waves, Geophysics and Machine Learning experts, G2Net was also released as a Kaggle competition. I'm pretty sure that you have heard about the discovery of Gravitational Waves (GW), signals from colliding binary black holes, back in 2016. If not, you can refresh your memory with [The Sound of Two Black Holes Colliding](https://www.youtube.com/watch?v=QyDcTbR-kEA). \n","\n","The aim of this competition is to detect GW signals from the mergers of binary black holes. Specifically, the participant was expected to build a model to analyse synthetic GW time-series data from a network of Earth-based detectors (LIGO Hanford, LIGO Livingston and Virgo). The [Data](https://www.kaggle.com/c/g2net-gravitational-wave-detection/data) was simulated with a sampling rate of 2048 Hz. Each of the time-series originated by the corresponding detector comprises a channel (three in total).\n","\n","In the context of this notebook, data has been already standardised (with training set mean and standard deviation), transposed (to ease channels last format) and saved to TensorFlow Records format. Such data can be found in my Kaggle profile, [Training Dataset](https://www.kaggle.com/salbeal94/g2net-float32-train) and [Test Dataset](https://www.kaggle.com/salbeal94/g2net-float32-test). Since it is a classification task, the output is the black hole merger occurence probability."]},{"cell_type":"markdown","metadata":{"id":"9ZzaCoZThMfh"},"source":["### 1.2 Implemented Model"]},{"cell_type":"markdown","metadata":{"id":"CkfwBa3S-FgA"},"source":["After many iterations, the model used for the competition ended up being a 2D Convolutional Neural Network (CNN) preceded by a series of time-series processing techniques. Even the preprocessing has been here implemented as part of the model to stay loyal to the end-to-end philosophy. Such a model contains a series of building blocks several of them presented as trainable Tensorflow Keras layers. These are described as follows:\n","- **Tukey Window (trainable/non-trainable)**: Introduces a tappering effect that forces the signal amplitude to decay until having zero values at the ends. It is applied to avoid artefacts stemming from discontinuities when taking Fourier transforms or similar.\n","\n","- **Bandpass Filter (trainable/non-trainable)**: Applies a filter with the frequency response of a Butterworth filter. The idea is to filter out or attenuate frequencies that have nothing to do with the merger.\n","\n","- **Constant-Q Transform (trainable/non-trainable)**: Transforms the time-domain signal into the time-frequency domain. In other words, it converts a time-domain signal to a spectrogram. Particularly, the PyTorch CQT1992v2 implementation from [nnAudio](https://github.com/KinWaiCheuk/nnAudio) has been taken as reference. It has been re-implemented in TensorFlow for being one of the most cost-effective solutions offered by the library. As an additional functionality, the layer keeps track of maximum spectrogram magnitudes and normalises its values to a preset range for the sake of stability. The output spectrogram is later resized with bilinear interpolation to adapt it to the downstream CNN recommended input sizes.\n","\n","- **Channel Permutation (non-trainable)**: Randomly decides whether to apply a stochastic permutation of the channels. The aim is to make the prediction a bit less independent of the detector it comes from, pressumably acting as a regularisation layer.\n","\n","- **Spectral Masking (non-trainable)**: Randomly decides whether to stochastically mask certain time or frequency bands. Similar to the permutation, the idea is for this layer to act as a regularising operation.\n","\n","- **Convolutional Neural Network (trainable/non-trainable)**: Used as backbone to extract the relevant features to be ingested by a single fully-connected neuron with sigmoid activation (after flattening). Given its performance-complexity trade-off in the ImageNet dataset, the EfficientNet family from [AutoML](https://github.com/google/automl) was selected as a more than appropriate model for this purpose."]},{"cell_type":"markdown","metadata":{"id":"-SsrWhul9ASn"},"source":["![G2Net Model](https://github.com/salvaba94/G2Net/blob/main/img/Model.png?raw=true \"G2Net Model\")"]},{"cell_type":"markdown","metadata":{"id":"f8B4eskr10Cg"},"source":["## 2. Configuration"]},{"cell_type":"markdown","metadata":{"id":"BB-pV4-c95z9"},"source":["### 2.1 Environment Configuration"]},{"cell_type":"markdown","metadata":{"id":"3-lnhaAD6Wo0"},"source":["Before starting with implementation-specific details, let's configure some aspects of the environment: \n","\n","- Make sure the Colab environment type is set to TPU going to ```Runtime → Change runtime type → TPU```\n","- Mount your Google Drive to save any output model after the execution.\n","- Install any library that might be missing from the defaults.\n","\n"]},{"cell_type":"code","metadata":{"id":"0i6EHThj5Oal","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634919591601,"user_tz":-120,"elapsed":15875,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}},"outputId":"7f9efd12-9c7c-4b2c-c5c9-86b10812387c"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\", force_remount = True)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"Lia9PRy4RtNd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634919600196,"user_tz":-120,"elapsed":8599,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}},"outputId":"68228b72-5077-4fdb-8816-4eda19f98e20"},"source":["!pip install tensorflow-addons\n","!pip install adabelief-tf\n","!git clone https://github.com/google/automl.git automl-efficientnetv2/automl"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 491 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 512 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 542 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 573 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 583 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 604 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 614 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 634 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 645 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 665 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 675 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 696 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 706 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 737 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 768 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 778 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 788 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 808 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 819 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 839 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 849 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 860 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 870 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 880 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 890 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 901 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 911 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 921 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 942 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 952 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 962 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 983 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 993 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.14.0\n","Collecting adabelief-tf\n","  Downloading adabelief_tf-0.2.1-py3-none-any.whl (6.4 kB)\n","Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.7/dist-packages (from adabelief-tf) (0.8.9)\n","Collecting colorama>=0.4.0\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from adabelief-tf) (2.6.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.1.2)\n","Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (2.6.0)\n","Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (5.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.12)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (0.12.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.1.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.12.1)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (0.2.0)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (2.6.0)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (0.4.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.6.3)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (0.37.0)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.15.0)\n","Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.41.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (3.17.3)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (3.1.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (3.7.4.3)\n","Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (2.6.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (3.3.0)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.19.5)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.0.0->adabelief-tf) (1.5.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (1.8.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (3.3.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (57.4.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (0.4.6)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (1.35.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (4.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf) (3.6.0)\n","Installing collected packages: colorama, adabelief-tf\n","Successfully installed adabelief-tf-0.2.1 colorama-0.4.4\n","Cloning into 'automl-efficientnetv2/automl'...\n","remote: Enumerating objects: 4011, done.\u001b[K\n","remote: Counting objects: 100% (398/398), done.\u001b[K\n","remote: Compressing objects: 100% (208/208), done.\u001b[K\n","remote: Total 4011 (delta 236), reused 300 (delta 188), pack-reused 3613\u001b[K\n","Receiving objects: 100% (4011/4011), 25.29 MiB | 33.81 MiB/s, done.\n","Resolving deltas: 100% (2988/2988), done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"mjfq4VuL_5-3"},"source":["### 2.2 Code Configuration "]},{"cell_type":"markdown","metadata":{"id":"SvBtCNP4AOd4"},"source":["\n","Now it is high time to start with the Python specifics. Run the following cell to import the necessary libraries for the code to work."]},{"cell_type":"code","metadata":{"id":"Q-DAIsqcRtNj","executionInfo":{"status":"ok","timestamp":1634919603895,"user_tz":-120,"elapsed":3706,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["import sys\n","import os\n","import warnings\n","import copy\n","import pandas as pd\n","import numpy as np\n","import multiprocessing as mp\n","import tensorflow as tf\n","import seaborn as sns\n","from tensorflow.data.experimental import AUTOTUNE\n","from tensorflow.python.client import device_lib\n","from tensorflow.keras.layers import Input, Dense, Flatten\n","from tensorflow.keras.layers.experimental.preprocessing import Resizing\n","from adabelief_tf import AdaBeliefOptimizer\n","from datetime import datetime\n","from pathlib import Path, os\n","from functools import partial\n","from scipy import signal\n","from scipy import interpolate\n","from typing import Tuple, Union, Mapping\n","import matplotlib.pyplot as plt\n","import librosa.display\n","\n","%matplotlib inline"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fDgrlDxGCgCW"},"source":["Set the configuration variables necessary for the code to run. Here is where you should come if you would like to play with the model. Don't worry if some of the variables are not clear enough at this point. Their utility can be derived as these are used all along the code.\n"]},{"cell_type":"code","metadata":{"id":"2NS3QU9s8I3A","executionInfo":{"status":"ok","timestamp":1634919603896,"user_tz":-120,"elapsed":5,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class Config:\n","### General data #############################################################\n","    N_SAMPLES, N_DETECT = 4096, 3\n","\n","### Training #################################################################\n","    FROM_TFR = False\n","    MODEL_TRAIN = True\n","    MODEL_SAVE_NAME = \"Model_Ref.h5\"\n","    MODEL_PRELOAD = False\n","    MODEL_PRELOAD_NAME = \"Model_Ref.h5\"\n","    HISTORY_NAME = \"history_train.csv\"\n","\n","    MODEL_PATH = Path(\"models\")\n","    CKPT_PATH = Path(\"checkpoints\")\n","\n","    SPLIT = 0.98\n","    SEED_SPLIT = 21\n","    BATCH_SIZE = 128\n","    BATCH_SIZE_TEST = 32\n","    EPOCHS = 1\n","    LEARNING_RATE = 0.0001\n","    \n","### Prediction ################################################################\n","    MODEL_PREDICT = True\n","    PREDICTIONS_NAME = \"submission.csv\"\n","\n","\n","### Model ####################################################################\n","    TUKEY_SHAPE = 0.25\n","    TRAINABLE_TUKEY = False\n","\n","    DEGREE_FILT = 6\n","    F_BAND_FILT = (20., 500.)\n","    TRAINABLE_FILT = True\n","\n","    SAMPLE_RATE = 2048\n","    F_BAND_SPEC = (20., 500.)\n","    HOP_LENGTH = 64\n","    BINS_PER_OCTAVE = 12\n","    WINDOW_CQT = \"hann\"\n","    TRAINABLE_CQT = False\n","    \n","    IMAGE_SIZE = 260\n","\n","    P_PERM = 0.1\n","\n","    P_MASK = 0.1\n","    N_MAX_MASK = 2\n","    W_MASK = (0, IMAGE_SIZE // 6)\n","    \n","    MODEL_ID = \"efficientnetv2-b2\"\n","    MODEL_ID_WEIGHTS = MODEL_ID\n","\n","### Plotting #################################################################\n","    PLOT_EXAMPLE = True\n","    PLOT_TEST = False"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DfL17IFSIvdX"},"source":["To make use of TPUs, data ingested by the model should be stored in Google Cloud Storage (GCS) buckets. Fortunately, Kaggle is so integrated with Google that public datasets get uploaded to Google Cloud. To know their bucket addresses one should just run the following in a Kaggle notebook:\n","```\n","from kaggle_datasets import KaggleDatasets\n","MODEL_BUCK = KaggleDatasets().get_gcs_path(\"automl-efficientnetv2-ckpt\")\n","TEST_PATH = KaggleDatasets().get_gcs_path(\"g2net-float32-test\")\n","TRAIN_PATH = KaggleDatasets().get_gcs_path(\"g2net-float32-train\")\n","```\n","\n","The output addresses should be pasted below as raw strings (check them from time to time as they are periodically changed)."]},{"cell_type":"code","metadata":{"id":"ci1FwjxuRtNo","executionInfo":{"status":"ok","timestamp":1634919603897,"user_tz":-120,"elapsed":5,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["os.environ[\"MODEL_BUCK\"] = r\"gs://kds-b512e196d236dcc25f915f875eadb7bcfda6c3ce473349146c8cc4a4\"\n","os.environ[\"TEST_BUCK\"] = r\"gs://kds-7f9c8748243b5c63ecfe7739ca8e1cdf5caa2106040be8c8ee1f603a\"\n","os.environ[\"TRAIN_BUCK\"] = r\"gs://kds-3e8541897910bfef73ed5b5f448a5bab5df453506aa6d94488411a24\"\n","MODEL_BUCK = os.environ[\"MODEL_BUCK\"]\n","TEST_BUCK = os.environ[\"TEST_BUCK\"]\n","TRAIN_BUCK = os.environ[\"TRAIN_BUCK\"]"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WQ3m8_mFNgGY"},"source":["Run the next cell to tune the paths and add AutoML EfficientNet model paths to Python path."]},{"cell_type":"code","metadata":{"id":"Qh3Yz_vwRtNx","executionInfo":{"status":"ok","timestamp":1634919603897,"user_tz":-120,"elapsed":4,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["OV_PATH = Path(\"/content\")\n","SRC_PATH = str(Path(OV_PATH, \"automl-efficientnetv2\"))\n","CKT_PATH = MODEL_BUCK + \"/\" + Config.MODEL_ID_WEIGHTS\n","TRAIN_PATH = TRAIN_BUCK + \"/train\"\n","TEST_PATH = TEST_BUCK + \"/test\"\n","\n","AUTOML_PATH = str(Path(SRC_PATH, \"automl\"))\n","EFFNETV2_PATH = str(Path(AUTOML_PATH, \"efficientnetv2\"))\n","\n","if SRC_PATH not in sys.path:\n","    sys.path.append(SRC_PATH)\n","    sys.path.append(AUTOML_PATH)\n","    sys.path.append(EFFNETV2_PATH)\n","    \n","from automl.efficientnetv2 import effnetv2_model\n","from automl.efficientnetv2 import hparams\n","from automl.efficientnetv2 import effnetv2_configs\n","from automl.efficientnetv2 import utils"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1vIVWrJQDWA9"},"source":["The following pair of cells automatically configures the TensorFlow strategy and device (relevant for TPU) and sets the data type to use."]},{"cell_type":"code","metadata":{"id":"gks0T4PcRtNl","executionInfo":{"status":"ok","timestamp":1634919603897,"user_tz":-120,"elapsed":3,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class Acceleration(object):\n","    \"\"\"\n","    General hardware acceleration class\n","    \"\"\"\n","\n","    @staticmethod\n","    def get_acceleration() -> None:\n","        \"\"\"\n","        Function to get and configure the hardware acceleration. It will \n","        sequentially try to configure TPU, GPU and CPU.\n","        \"\"\"\n","        try:\n","            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","            print(\"Device:\", tpu.master())\n","            tf.config.experimental_connect_to_cluster(tpu)\n","            tf.tpu.experimental.initialize_tpu_system(tpu)\n","            strategy = tf.distribute.TPUStrategy(tpu)\n","            device = \"TPU\"\n","\n","        except:\n","            strategy = tf.distribute.get_strategy()\n","            device =\"GPU\" if \"GPU\" in [d.device_type for d in \n","                                   device_lib.list_local_devices()] else \"CPU\"\n","\n","        print(device, \"Number of replicas:\", strategy.num_replicas_in_sync)\n","        return strategy, device"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HvpAB7BRtNm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634919621280,"user_tz":-120,"elapsed":17047,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}},"outputId":"44aeabbc-72be-48e9-f2f2-749d8e26a51a"},"source":["# Create strategy and define data types for data and tensorflow models\n","strategy, device = Acceleration.get_acceleration()\n","dtype = tf.float32 # Do not modify since TF Records are in tf.float32"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: grpc://10.2.221.114:8470\n","INFO:tensorflow:Clearing out eager caches\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Clearing out eager caches\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.2.221.114:8470\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.2.221.114:8470\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Finished initializing TPU system.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Finished initializing TPU system.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Found TPU system:\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Found TPU system:\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["TPU Number of replicas: 8\n"]}]},{"cell_type":"markdown","metadata":{"id":"f4NEqmyYKK8W"},"source":["Since each TF Record contains several examples (each with ID, time-domain signals and label if any), one might lose track of the number of examples in training and test sets. Actually, it needs to be known to figure out the number of steps per epoch. Such information is contained in two CSV that need to be locally stored. This is the purpose of the following lines."]},{"cell_type":"code","metadata":{"id":"r1rNKp8dV_xB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634919624614,"user_tz":-120,"elapsed":3343,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}},"outputId":"97d9b98c-5626-43b1-e357-572654711027"},"source":["!mkdir -p g2net-float32-train\n","!mkdir -p g2net-float32-test\n","!gsutil cp $TRAIN_BUCK/training_labels.csv g2net-float32-train\n","!gsutil cp $TEST_BUCK/sample_submission.csv g2net-float32-test"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Copying gs://kds-3e8541897910bfef73ed5b5f448a5bab5df453506aa6d94488411a24/training_labels.csv...\n","/ [1 files][  6.9 MiB/  6.9 MiB]                                                \n","Operation completed over 1 objects/6.9 MiB.                                      \n","Copying gs://kds-7f9c8748243b5c63ecfe7739ca8e1cdf5caa2106040be8c8ee1f603a/sample_submission.csv...\n","/ [1 files][  3.2 MiB/  3.2 MiB]                                                \n","Operation completed over 1 objects/3.2 MiB.                                      \n"]}]},{"cell_type":"markdown","metadata":{"id":"t-Umm9pCPllE"},"source":["## 3. Source Code"]},{"cell_type":"markdown","metadata":{"id":"m0nvhKgtP3rD"},"source":["### 3.1 Utilities"]},{"cell_type":"markdown","metadata":{"id":"byyHIs_AQeav"},"source":["This section defines a pair of utility classes for general usage or plotting purposes. Sad as it is, there is nothing that should be really highlighted from here."]},{"cell_type":"code","metadata":{"id":"u3dtYM1ERtNx","executionInfo":{"status":"ok","timestamp":1634919624614,"user_tz":-120,"elapsed":3,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class GeneralUtilities(object):\n","    \"\"\"\n","    General utilities class\n","    \"\"\"\n","\n","    @staticmethod\n","    def broadcast_dim(\n","            x: tf.Tensor\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Auto broadcast input for tensorflow.\n","\n","        Parameters\n","        ----------\n","        x : tf.Tensor\n","            Broadcasted tensor.\n","\n","        Raises\n","        ------\n","        ValueError\n","            If input shape is not 1, 2 or 3.\n","\n","        Returns\n","        -------\n","        x : tf.Tensor\n","            Broadcasted tensor.\n","        \"\"\"\n","        rank = len(x.get_shape().as_list())\n","\n","        if rank == 1:\n","            x = tf.expand_dims(tf.expand_dims(x, axis = 0), axis = -1)\n","        elif rank == 2:\n","            x = tf.expand_dims(x, axis = -1)\n","        elif rank == 3:\n","            pass\n","        else:\n","            raise ValueError(\"Only support input with shape = (n_batch, n_samples) \\\n","                             or shape = (n_samples)\")\n","        return x"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"MsbQxBKgQLb5","executionInfo":{"status":"ok","timestamp":1634919624614,"user_tz":-120,"elapsed":2,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class PlottingUtilities(object):\n","    \"\"\"\n","    Plotting utilities class\n","    \"\"\"\n","\n","    time_tag = \"Time [s]\"\n","    freq_tag = \"Frequency [Hz]\"\n","    mag_tag = \"Strain [-]\"\n","    detector = (\"LIGO Hanford\", \"LIGO Livingston\", \"Virgo\")\n","\n","\n","    @classmethod\n","    def plot_wave(\n","            cls,\n","            waveforms: np.ndarray, \n","            timespan: float = 2.\n","        ) -> None:\n","        \"\"\"\n","        Function to plot waves from the 3 detectors.\n","\n","        Parameters\n","        ----------\n","        waveforms : np.ndarray, shape = (n_samples, n_channels)\n","            Waveform data to plot.\n","        timespan : float, optional\n","            Time span of the waveforms [s]. The default is 2.\n","        \"\"\"\n","        if waveforms.shape[-1] != 3:\n","            raise ValueError(\"Function expects exactly data for 3 detectors\")\n","\n","        time = np.linspace(0., timespan, waveforms.shape[0]) [:, np.newaxis]\n","        min_val, max_val = waveforms.min(), waveforms.max()\n","        dataframe = pd.DataFrame(data = np.hstack((time, waveforms)), \n","                                 columns = [cls.time_tag, \n","                                            cls.mag_tag + \" \" + cls.detector[0], \n","                                            cls.mag_tag + \" \" + cls.detector[1], \n","                                            cls.mag_tag + \" \" + cls.detector[2]])\n","\n","        plt.style.use(\"seaborn\")\n","        fig, axes = plt.subplots(3, 1, figsize = (15, 10))\n","        for i in range(len(cls.detector)):\n","            sns.lineplot(data = dataframe, x = cls.time_tag, \n","                         y = cls.mag_tag + \" \" + cls.detector[i], ax = axes[i])\n","            axes[i].legend([cls.detector[i]])\n","            axes[i].set_ylabel(cls.mag_tag)\n","            axes[i].set_ylim(min_val, max_val)\n","    \n","\n","    @classmethod\n","    def plot_spectrogram(\n","            cls,\n","            spectrogram: np.ndarray,\n","            **kwargs\n","        ) -> None:\n","        \"\"\"\n","        Function to plot a spectrogram.\n","\n","        Parameters\n","        ----------\n","        spectrogram : np.ndarray, shape = (n_freq, n_time, n_channels)\n","            Spectrogram data.\n","        \"\"\"\n","        if spectrogram.shape[-1] != 3:\n","            raise ValueError(\"Function expects exactly data for 3 detectors\")\n","\n","        plt.style.use(\"seaborn\")\n","        fig, axes = plt.subplots(1, len(cls.detector), figsize = (15, 5))\n","        for i in range(len(cls.detector)):\n","            librosa.display.specshow(data = spectrogram[..., i], \n","                                     ax = axes[i], **kwargs)\n","            axes[i].set_xlabel(cls.time_tag)\n","            axes[i].set_ylabel(cls.freq_tag)\n","            axes[i].set_title(cls.detector[i])\n","      "],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iH3UjvBRM1lj"},"source":["### 3.2 Data Ingestion"]},{"cell_type":"markdown","metadata":{"id":"BGxKmWGJNQ9O"},"source":["The following class implements the logic to serialise/deserialise TensorFlow Records for this problem. A target argument controls whether the dataset is used for training/validation or test/prediction. The examples contained should be deserialised by creating a TensorFlow Datasets pipeline."]},{"cell_type":"code","metadata":{"id":"hEqGax6XMyQL","executionInfo":{"status":"ok","timestamp":1634919625078,"user_tz":-120,"elapsed":466,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class TFRDatasetCreator(object):\n","    \"\"\"\n","    Class to aid in the creation of tensorflow records datasets\n","    \"\"\"\n","\n","    def __init__(\n","            self,\n","            dataframe: pd.DataFrame,\n","            datadir: Path,\n","            trans: bool = False,\n","            data_stats: Tuple[float, float] = None,\n","            raw_dir: bool = False,\n","            ext_in: str = \".npy\",\n","\n","        ) -> None:\n","        \"\"\"\n","        Function to initialise the object.\n","\n","        Parameters\n","        ----------\n","        dataframe : pd.DataFrame, columns = (id, targets)\n","            Dataframe with the indeces of the samples.\n","        datadir : Path\n","            Data directory.\n","        trans : bool, optional\n","            Whether the to transpose the data before storing. The default is \n","            False.\n","        data_stats : Tuple[float, float], optional\n","            If provided, these are used to standardise the input data. It \n","            contains mean and standard deviation in this order. The default is None.\n","        raw_dir : bool\n","            Whether the folder should be treated as a raw data folder directory.\n","            The default is False.\n","        ext_in : str, optional\n","            Extension of the input files. The default is \".npy\".\n","        \"\"\"\n","\n","        self.df = dataframe.copy()\n","        self.datadir = datadir\n","        self.data_stats = data_stats\n","        self.trans = trans\n","\n","        if raw_dir:\n","            self.df[\"path\"] = str(datadir) + os.sep + dataframe[\"id\"].apply(\n","                lambda x: x[0]) + os.sep + dataframe[\"id\"].apply(\n","                lambda x: x[1]) + os.sep + dataframe[\"id\"].apply(\n","                lambda x: x[2]) + os.sep + dataframe[\"id\"].astype(str) + ext_in\n","        else:\n","            self.df[\"path\"] = str(datadir) + os.sep + dataframe[\"id\"].astype(str) + ext_in\n","\n","\n","    @staticmethod\n","    def _bytes_feature(\n","            value: Union[np.ndarray, tf.Tensor, str]\n","        ) -> tf.train.Feature:\n","        \"\"\"\n","        Function to convert an array, string or tensor to a list of bytes feature\n","\n","        Parameters\n","        ----------\n","        value : Union[np.ndarray, tf.Tensor, str]\n","            Input value to convert to a list of bytes feature.\n","\n","        Returns\n","        -------\n","        tf.train.Feature\n","            The converted feature.\n","        \"\"\"\n","\n","        if isinstance(value, type(tf.constant(0))):\n","            value = value.numpy()\n","        return tf.train.Feature(bytes_list = tf.train.BytesList(value = [value]))\n","\n","\n","    @staticmethod\n","    def _int_feature(\n","            value: Union[int, bool]\n","        ) -> tf.train.Feature:\n","        \"\"\"\n","        Function to convert a bool / enum / int / uint to an int feature.\n","        \n","        Parameters\n","        ----------\n","        value : Union[int, bool]\n","            Input value to convert to an int feature.\n","        \n","        Returns\n","        -------\n","        tf.train.Feature\n","            The converted feature.\n","        \"\"\"\n","\n","        return tf.train.Feature(int64_list = tf.train.Int64List(value = [value]))\n","\n","\n","    def _serialize_example(\n","            self, \n","            idx: int,\n","            dtype: type = tf.float32\n","        ) -> str:\n","        \"\"\"\n","        Method to serialise a single example.\n","        \n","        Parameters\n","        ----------\n","        idx : int\n","            ID of the example to selialize.\n","        dtype : type, optional\n","            Data type to which the example should be serialised. The default \n","            is tf.float32.\n","\n","        Returns\n","        -------\n","        str\n","            Serialised example.\n","        \"\"\"\n","\n","        data = np.load(self.df[\"path\"][idx])\n","        identity = self.df[\"id\"][idx]\n","        target = self.df[\"target\"][idx]\n","\n","        data = data.T if self.trans else data\n","        \n","        if self.data_stats is not None:\n","            data = (data - self.data_stats[0]) / self.data_stats[-1] \n","\n","        data = tf.convert_to_tensor(data, dtype = dtype)\n","\n","        feature = {\n","            \"data\": self._bytes_feature(tf.io.serialize_tensor(data)),\n","            \"id\": self._bytes_feature(identity.encode()),\n","            \"target\": self._int_feature(np.int(target))\n","        }\n","        \n","        example = tf.train.Example(features = tf.train.Features(feature = feature))\n","        return example.SerializeToString()\n","\n","\n","    def _serialize_batch(\n","            self,\n","            data: Tuple[int, np.ndarray],\n","            destdir: Path,\n","            dtype: type = tf.float32,\n","            filename: str = \"train\",\n","            ext_out: str = \".tfrec\"\n","        ) -> None:\n","        \"\"\"\n","        Method to serialise a batch of examples and write it to tensorflow record.\n","        \n","        Parameters\n","        ----------\n","        data : Tuple[int, np.ndarray]\n","            Batch data. Contains batch ID and array of examples to be serialised,\n","            respectively.\n","        destdir : Path\n","            Destination directory.\n","        dtype : type, optional\n","            Data type to which the examples should be serialised. The default \n","            is tf.float32.\n","        filename : str, optional\n","            Filename of the output tensorflow record. To it, the batch ID and \n","            the range of the examples contained in it will be appended. \n","            The default is \"train\".\n","        ext_out : str, optional\n","            Extension of the output files. The default is \".tfrec\".\n","        \"\"\"\n","\n","        n_batch, batch = data\n","        filename_batch = destdir.joinpath(filename + str(n_batch).zfill(3) \n","                                          + \"-\" + str(batch.shape[0]) + ext_out)\n","        with tf.io.TFRecordWriter(str(filename_batch)) as writer:\n","            for idx in batch:\n","                writer.write(self._serialize_example(idx, dtype = dtype))\n","\n","\n","\n","    def serialize_dataset(\n","            self,\n","            n_samples,\n","            destdir: Path,\n","            dtype: type = tf.float32,\n","            filename: str = \"train\",\n","            ext_out: str = \".tfrec\"\n","        ) -> None:\n","        \"\"\"\n","        Method to serialise a the full dataset and write it to a set of \n","        tensorflow records.\n","        \n","        Parameters\n","        ----------\n","        n_samples : int\n","            Number of samples used to calculate the number of tensorflow \n","            records files.\n","        destdir : Path\n","            Destination directory.\n","        dtype : type, optional\n","            Data type to which the examples should be serialised. The default \n","            is tf.float32.\n","        filename : str, optional\n","            Filename of the output tensorflow record. The default is \"train\".\n","        ext_out : str, optional\n","            Extension of the output files. The default is \".tfrec\".\n","        \"\"\"\n","\n","        destdir.mkdir(parents = True, exist_ok = True)\n","\n","        n_files = np.int32(np.ceil(self.df.shape[0] / n_samples))\n","\n","        for n_batch, batch in enumerate(np.array_split(self.df.index, n_files)): \n","            print(\"Writing TFRecord \" + str(n_batch) + \" with files from \" + \n","                  str(batch[0]) + \" to \" + str(batch[-1]))\n","            writer = partial(self._serialize_batch, destdir = destdir, \n","                         dtype = dtype, filename = filename, ext_out = ext_out)\n","            writer((n_batch, batch))\n","\n","\n","    @staticmethod\n","    def deserialize_example(\n","            element,\n","            dtype: type = tf.float32,\n","            target: bool = False,\n","            shape: Tuple[int, int] = (4096, 3)\n","        ) -> Tuple[tf.Tensor, int]:\n","        \"\"\"\n","        Method intended to be used in any dataset generator to deserialise the \n","        examples from tensorflow records.\n","        \n","        Parameters\n","        ----------\n","        element : int\n","            Serialised example.\n","        dtype : type, optional\n","            Data type to which the example was serialised. The default is tf.float32.\n","        target : bool, optional\n","            Whether the label should be included in the output. If false, it is \n","            interpreted that the aim is to predict and, therefore, the ID of the \n","            example is appended instead. The default is False.\n","        shape : Tuple[int, int], optional\n","            Shape to which the example data should be reshaped \n","        \n","        Returns\n","        -------\n","        Tuple[tf.Tensor, int]\n","            Example data and label or ID, depending on the value of target argument.\n","        \"\"\"\n","\n","        feature = {\n","            \"data\"  : tf.io.FixedLenFeature([], tf.string),\n","            \"id\" : tf.io.FixedLenFeature([], tf.string),\n","            \"target\" : tf.io.FixedLenFeature([], tf.int64)\n","        }\n","\n","        content = tf.io.parse_single_example(element, feature)\n"," \n","        data = content[\"data\"]\n","        data = tf.io.parse_tensor(data, out_type = dtype)\n","        data = tf.reshape(data, shape = shape)\n","\n","        idx = tf.cast(tf.strings.unicode_decode(content[\"id\"], \"UTF-8\"), \n","                      dtype = tf.int32)\n","        label = tf.cast(content[\"target\"] , dtype)\n","        \n","        if target:\n","            return data, label\n","        else:\n","            return data, idx"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ejvd2z2gSGz9","executionInfo":{"status":"ok","timestamp":1634919625079,"user_tz":-120,"elapsed":3,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class DatasetGeneratorTF(object):\n","    \"\"\"\n","    Class to aid in the creation of dataset pipelines using tensorflow\n","    \"\"\"\n","\n","    def __init__(\n","            self,\n","            dataframe: pd.DataFrame,\n","            datadir: Path,\n","            batch_size: int = 64,\n","            dtype: type = tf.float32,\n","            raw_dir: bool = False,\n","            ext: str = \".tfrec\"\n","        ) -> None:\n","        \"\"\"\n","        Function to initialise the object.\n","\n","        Parameters\n","        ----------\n","        dataframe : pd.DataFrame, columns = (id, targets)\n","            Dataframe with the indeces of the samples.\n","        datadir : Path\n","            Data directory.\n","        batch_size : int, optional\n","            Batch size. The default is 32.\n","        dtype : type, optional\n","            Data type to use. The default is np.float16.\n","        raw_dir : bool\n","            Whether the folder should be treated as a raw data folder directory.\n","            The default is False.\n","        ext : str, optional\n","            Extension of the files. The default is \".tfrec\".\n","        \"\"\"\n","\n","        self.df = dataframe.copy()\n","        self.datadir = datadir\n","        self.batch_size = batch_size\n","        self.dtype = dtype\n","\n","        if raw_dir:\n","            self.df[\"path\"] = str(datadir) + os.sep + dataframe[\"id\"].apply(\n","                lambda x: x[0]) + os.sep + dataframe[\"id\"].apply(\n","                lambda x: x[1]) + os.sep + dataframe[\"id\"].apply(\n","                lambda x: x[2]) + os.sep + dataframe[\"id\"].astype(str) + ext\n","        else:\n","            self.df[\"path\"] = str(datadir) + os.sep + dataframe[\"id\"].astype(str) + ext\n","\n","\n","    def _read_npy(\n","            self, \n","            filename: Path\n","        ) -> np.ndarray:\n","        \"\"\"\n","        Auxiliary method to read data from npy file.\n","\n","        Parameters\n","        ----------\n","        filename : Path\n","            Path and name of the example to read.\n","\n","        Returns\n","        -------\n","        np.ndarray\n","            Data from the file.\n","        \"\"\"\n","\n","        example_data = tf.cast(np.load(filename), self.dtype)\n","        return example_data\n","\n","\n","    def _get_dataset_from_npy(\n","            self,\n","            shuffle: bool = True,\n","            buffer_size: int = 1024,\n","            repeat: bool = True,\n","            target: bool = True\n","        ) -> tf.data.Dataset:\n","        \"\"\"\n","        Function to get the dataset pipeline from npy files.\n","\n","        Parameters\n","        ----------\n","        shuffle : bool, optional\n","            Whether to add shuffle to the pipeline or not. The default is True.\n","        buffer_size : int, optional\n","            Shuffle buffer size. The default is 1024.\n","        repeat : bool, optional\n","            Whether to repeat the dataset or not. The default is True\n","        target : bool, optional\n","            Whether to add the label or not. The default is True.\n","\n","        Returns\n","        -------\n","        tf.data.Dataset\n","            Dataset pipeline.\n","        \"\"\"\n","\n","        feature_ds = tf.data.Dataset.from_tensor_slices(self.df[\"path\"])\n","        feature_ds = feature_ds.map(lambda x: tf.numpy_function(\n","            self._read_npy, [x], self.dtype), \n","            num_parallel_calls = AUTOTUNE)\n","\n","        if target:\n","            label_ds = tf.data.Dataset.from_tensor_slices(self.df[\"target\"])\n","            ds = tf.data.Dataset.zip((feature_ds, label_ds))\n","        else:\n","            ds = feature_ds\n","\n","        if shuffle:\n","            ds = ds.shuffle(buffer_size)\n","        \n","        if repeat:\n","            ds = ds.repeat()\n","\n","        ds = ds.batch(self.batch_size)\n","        return ds.prefetch(AUTOTUNE)\n","\n","\n","    def _get_dataset_from_tfrec(\n","            self,\n","            shuffle: bool = True,\n","            buffer_size: int = 1024,\n","            ordered: bool = False,\n","            repeat: bool = True,\n","            target: bool = True\n","        ) -> tf.data.Dataset:\n","        \"\"\"\n","        Function to get the dataset pipeline from tensorflow records.\n","\n","        Parameters\n","        ----------\n","        shuffle : bool, optional\n","            Whether to add shuffle to the pipeline or not. The default is True.\n","        buffer_size : int, optional\n","            Shuffle buffer size. The default is 1024.\n","        ordered: bool, optional\n","            Indicate whether the order matters when reading. The default is False.\n","        repeat : bool, optional\n","            Whether to repeat the dataset or not. The default is True.\n","        target : bool, optional\n","            Whether to add the label or not. The default is True.\n","        \n","        Returns\n","        -------\n","        tf.data.Dataset\n","            Dataset pipeline.\n","        \"\"\"\n","\n","        ds = tf.data.TFRecordDataset(self.df[\"path\"], num_parallel_reads = AUTOTUNE)\n","\n","        if not ordered:\n","            ignore_order = tf.data.Options()\n","            ignore_order.experimental_deterministic = False\n","            ds = ds.with_options(ignore_order)\n","\n","        ds = ds.map(lambda x: TFRDatasetCreator.deserialize_example(\n","            x, dtype = self.dtype, target = target, shape = (Config.N_SAMPLES, \n","            Config.N_DETECT)), num_parallel_calls = AUTOTUNE)\n","\n","        if shuffle:\n","            ds = ds.shuffle(buffer_size)\n","\n","        if repeat:\n","            ds = ds.repeat()\n","        \n","        ds = ds.batch(self.batch_size, drop_remainder = target)\n","        return ds.prefetch(AUTOTUNE)\n","\n","\n","    def get_dataset(\n","            self,\n","            tfrec: bool = True, \n","            shuffle: bool = True,\n","            buffer_size: int = 1024,\n","            ordered: bool = False,\n","            repeat: bool = True,\n","            target: bool = True\n","        ) -> tf.data.Dataset:\n","        \"\"\"\n","        Function to get the dataset pipeline.\n","\n","        Parameters\n","        ----------\n","        tfrec : bool, optional\n","            Whether TFRecord should be read or not. The default is True.\n","        shuffle_buffer : int, optional\n","            Shuffle buffer size. If None, no shuffle will be applied. The default \n","            is 3200.\n","        ignore_order : bool, optional\n","            Dataframe with the indeces of the samples. The default is True.\n","\n","        Returns\n","        -------\n","        tf.data.Dataset\n","            Dataset pipeline.\n","        \"\"\"\n","        \n","        ret_val = self._get_dataset_from_tfrec(shuffle, buffer_size, ordered,\n","            repeat, target) if tfrec else self._get_dataset_from_npy(\n","            shuffle, buffer_size, repeat, target)\n","        return ret_val"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cO4kSASvSOfl"},"source":["### 3.3 Custom Model"]},{"cell_type":"markdown","metadata":{"id":"ZatnDP9hX7f-"},"source":["#### 3.3.1 Custom Preprocessing Layers"]},{"cell_type":"markdown","metadata":{"id":"DMOLx0scSgTI"},"source":["The following lines implement the building blocks of the model that are not built-in in TensorFlow or provided by third parties. In the case of preprocessing layers, the Tukey window and the bandpass filter are included."]},{"cell_type":"code","metadata":{"id":"eWlRSAHVSf0l","executionInfo":{"status":"ok","timestamp":1634919625079,"user_tz":-120,"elapsed":3,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class TukeyWinLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    Layer that applies a Tukey window function to an input time series, where \n","    the possibility of training the shape parameter is given. Not usable with TPU.\n","    \"\"\"\n","\n","    def __init__(\n","            self, \n","            initial_alpha: float = 0.25,\n","            **kwargs\n","        ) -> None:\n","        \"\"\"\n","        Function to initialise the object.\n","\n","        Parameters\n","        ----------\n","        initial_alpha : float, optional\n","            Shape parameter of the tukey window. The default is 0.25.\n","        \"\"\"\n","    \n","        super(TukeyWinLayer, self).__init__(**kwargs)\n","\n","        self.alpha = tf.Variable(initial_value = initial_alpha, \n","                                 trainable = self.trainable,\n","                                 name = self.name + \"/alpha\", \n","                                 dtype = self.dtype)\n","\n","\n","    def build(\n","            self, \n","            input_shape: Tuple[int, int]\n","        ) -> None:\n","        \"\"\"\n","        Function to build the graph of the layer. Adds trainable and non-\n","        trainable parameters.\n","\n","        Parameters\n","        ----------\n","        input_shape : Tuple[int, int]\n","            Shape of the input to the layer.\n","        \"\"\"\n","\n","        if self.trainable:\n","            self.trainable_weights.append(self.alpha)\n","        else:\n","            self.non_trainable_weights.append(self.alpha)\n","        super(TukeyWinLayer, self).build(input_shape)\n","\n","\n","    def call(\n","            self, \n","            data: tf.Tensor\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Forward pass of the layer.\n","\n","        Parameters\n","        ----------\n","        data : tf.Tensor, shape = (None, n_samples, n_detectors)\n","            A batch of input waveforms, n_detectors (n_channels) should \n","            be last.\n","\n","        Returns\n","        -------\n","        tf.Tensor, shape = (None, n_samples, n_detectors)\n","            The corresponding batch of windowed signals.\n","        \"\"\"\n","\n","        x = GeneralUtilities.broadcast_dim(data)\n","        x = tf.cast(x, self.dtype)\n","        w_len = tf.shape(x)[1]\n","        window = GeneralUtilities.broadcast_dim(self._get_window(w_len))\n","        x *= window\n","        return x\n","\n","\n","    def _get_ones(\n","            self,\n","            w_len: int\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Case for a null shape parameter.\n","\n","        Parameters\n","        ----------\n","        w_len : int\n","            Length of the window.\n","\n","        Returns\n","        -------\n","        tf.Tensor : shape = (window_len,)\n","            A window of all ones.\n","        \"\"\"\n"," \n","        window = tf.ones(w_len, dtype = self.dtype)\n","        return window\n","\n","        \n","    def _get_hann(\n","            self,\n","            w_len: int\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Case for a unity shape parameter.\n","\n","        Parameters\n","        ----------\n","        w_len : int\n","            Length of the window.\n","\n","        Returns\n","        -------\n","        tf.Tensor : shape = (window_len,)\n","            A hann window.\n","        \"\"\"\n"," \n","        window = tf.signal.hann_window(w_len, periodic = False)\n","        window = tf.cast(window, dtype = self.dtype)\n","        return window\n","\n","\n","    def _get_tukey(\n","            self,\n","            w_len: int\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Case for a non-null and non-unity shape parameter.\n","\n","        Parameters\n","        ----------\n","        w_len : int\n","            Length of the window.\n","\n","        Returns\n","        -------\n","        tf.Tensor : shape = (window_len,)\n","            A tukey window.\n","        \"\"\"\n"," \n","        w_len_f = tf.cast(w_len, self.dtype)\n","       \n","        n = tf.range(0, w_len)\n","        width = tf.math.floor(self.alpha * (w_len_f - 1.)/2.)\n","        width = tf.minimum(width, w_len_f)\n","        width = tf.maximum(width, 0.)\n","        width = tf.cast(width, dtype = tf.int32)\n","\n","        n_1 = tf.cast(n[:width + 1], dtype = self.dtype)\n","        n_2 = tf.cast(n[width + 1: w_len - width - 1], dtype = self.dtype)\n","        n_3 = tf.cast(n[w_len - width - 1:], dtype = self.dtype)\n","    \n","        window_1 = 0.5 * (1. + tf.math.cos(np.pi * (\n","            -1. + 2. * n_1 / self.alpha / (w_len_f - 1.))))\n","        window_2 = tf.ones(tf.shape(n_2))\n","        window_3 = 0.5 * (1. + tf.math.cos(np.pi * (\n","            - 2./self.alpha + 1. + 2. * n_3 / self.alpha / (w_len_f - 1.))))\n","    \n","        window = tf.concat((window_1, window_2, window_3), axis = 0)\n","        return window\n","    \n","    \n","    def _get_window(\n","            self,\n","            w_len: int\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Tukey window getter.\n","\n","        Parameters\n","        ----------\n","        w_len : int\n","            Length of the window.\n","\n","        Returns\n","        -------\n","        tf.Tensor : shape = (window_len,)\n","            A tukey window handling the cases of null or unity shape parameters.\n","        \"\"\"\n","\n","        get_ones = partial(self._get_ones, w_len = w_len + 1)\n","        get_hann = partial(self._get_hann, w_len = w_len + 1)\n","        get_tukey = partial(self._get_tukey, w_len = w_len + 1)\n","\n","        window = tf.case([(tf.less_equal(self.alpha, 0.), get_ones),\n","                          (tf.greater_equal(self.alpha, 1.), get_hann)], \n","                         default = get_tukey)\n","        window = window[:-1]\n","        return window"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"xtTTf1u1WNTn","executionInfo":{"status":"ok","timestamp":1634919625079,"user_tz":-120,"elapsed":2,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class WindowingLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    Layer that applies a window function to an input time series.\n","    \"\"\"\n","\n","    def __init__(\n","            self, \n","            window: Union[str, Tuple[str, float]] = (\"tukey\", 0.1),\n","            window_len: int = 4096,\n","            **kwargs\n","        ) -> None:\n","        \"\"\"\n","        Function to initialise the object.\n","\n","        Parameters\n","        ----------\n","        window : str or Tuple[str, float], optional\n","            The type of window to create with any parameter it might need. \n","            The default is \"tukey\" with alpha 0.1.\n","        window_len : int, optional\n","            The number of samples in the window (set it to the signal length). \n","            The default is 4096.\n","        \"\"\"\n","    \n","        super(WindowingLayer, self).__init__(**kwargs)\n","\n","        self.window = window\n","        self.window_len = window_len\n","\n","        sig = signal.get_window(window, window_len)[np.newaxis, :, np.newaxis]\n","        self.window = tf.Variable(initial_value = sig, trainable = self.trainable,\n","                                  name = self.name + \"/window\", dtype = self.dtype)\n","\n","\n","    def build(\n","            self, \n","            input_shape: Tuple[int, int]\n","        ) -> None:\n","        \"\"\"\n","        Function to build the graph of the layer. Adds trainable and non-\n","        trainable parameters.\n","        \n","        Parameters\n","        ----------\n","        input_shape : Tuple[int, int]\n","            Shape of the input to the layer.\n","        \"\"\"\n","\n","        if self.trainable:\n","            self.trainable_weights.append(self.window)\n","        else:\n","            self.non_trainable_weights.append(self.window)\n","        super(WindowingLayer, self).build(input_shape)\n","\n","\n","    def call(\n","            self, \n","            data: tf.Tensor\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Forward pass of the layer.\n","\n","        Parameters\n","        ----------\n","        data : tf.Tensor, shape = (None, n_samples, n_detectors)\n","            A batch of input mono waveforms, n_detectors (n_channels) should \n","            be last.\n","\n","        Returns\n","        -------\n","        tf.Tensor, shape = (None, n_samples, n_detectors)\n","            The corresponding batch of windowed waveforms.\n","        \"\"\"\n","\n","        x = GeneralUtilities.broadcast_dim(data)\n","        x = tf.cast(x, self.dtype)\n","        x *= self.window\n","        return x\n","\n","\n","    def get_config(\n","            self\n","        ) -> Mapping[str, float]:\n","        \"\"\"\n","        Function to get the configuration parameters of the object.\n","        \n","        Returns\n","        -------\n","        Mapping[str, float]\n","            Dictionary containing the configuration parameters of the object.\n","        \"\"\"\n","        config = {\n","            \"window\" : self.window,\n","            \"window_len\" : self.window_len\n","        }\n","        \n","        config.update(super(WindowingLayer, self).get_config())\n","        return config"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"AZJeeK4TXtCG","executionInfo":{"status":"ok","timestamp":1634919625535,"user_tz":-120,"elapsed":5,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class BandpassLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    Layer that applies a bandpass Butterworth filter in the frequency domain, \n","    where the possibility of training frequency response from the filter is \n","    given.\n","    \"\"\"\n","\n","    def __init__(\n","            self, \n","            sample_rate: float = 2048.,\n","            degree: int = 8,\n","            f_band: Tuple[float, float] = (20., 500.),\n","            n_samples: int = 4096,\n","            **kwargs\n","        ) -> None:\n","        \"\"\"\n","        Funtion to initialize the object.\n","\n","        Parameters\n","        ----------\n","        sample_rate : float, optional\n","            The sampling rate for the input time series [Hz]. It is used to \n","            calculate the correct \"f_min\" and \"f_max\". The default is 2048.\n","        degree : int, optional\n","            Degree of the Butterworth filter. The default is 8.\n","        f_band : Tuple[float, float], optional\n","            The frequency band for the bandpass filter [Hz]. \n","            The default is (20., 500). \n","        n_samples : int, optional\n","            Number of samples of the signal to filter. The default is 4096.\n","        \"\"\"\n","    \n","        super(BandpassLayer, self).__init__(**kwargs)\n","        \n","        self.sample_rate = sample_rate\n","        self.degree = degree\n","        self.f_band = f_band\n","        self.n_samples = n_samples\n","\n","        if f_band[-1] <= f_band[0]:\n","            raise ValueError(\"Maximum frequency in spectral band should be \\\n","                             higher than minimum frequency\")\n","\n","        f_nyq = sample_rate / 2.\n","        f_min = f_band[0] / f_nyq\n","        f_max = f_band[-1] / f_nyq\n","        self.norm = tf.constant(np.sqrt((f_band[-1] - f_band[0]) / f_nyq), \n","                                dtype = self.dtype)\n","\n","        f_fft = np.fft.rfftfreq(n_samples, d = 1./sample_rate)\n","\n","        b, a = signal.butter(degree, (f_min, f_max), btype = \"bandpass\")\n","        w, gain = signal.freqz(b, a, worN = 2 * f_fft.shape[0])\n","        f = (f_nyq / np.pi) * w\n","        gain = np.abs(gain)\n","\n","        gain_f = interpolate.interp1d(f, gain, fill_value = \"extrapolate\")\n","        f_response = gain_f(f_fft)\n","        f_response = f_response[np.newaxis, :]\n","\n","        self.f_response = tf.Variable(initial_value = f_response, \n","                                 trainable = self.trainable,\n","                                 name = self.name + \"/f_response\", \n","                                 dtype = self.dtype)\n","\n","\n","    def build(\n","            self, \n","            input_shape: Tuple[int, int]\n","        ) -> None:\n","        \"\"\"\n","        Function to build the graph of the layer. Adds trainable and non-\n","        trainable parameters.\n","\n","        Parameters\n","        ----------\n","        input_shape : Tuple[int, int]\n","            Shape of the input to the layer.\n","        \"\"\"\n","\n","        if self.trainable:\n","            self.trainable_weights.append(self.f_response)\n","        else:\n","            self.non_trainable_weights.append(self.f_response)\n","        super(BandpassLayer, self).build(input_shape)\n","\n","\n","    def call(\n","            self, \n","            data: tf.Tensor\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Forward pass of the layer.\n","\n","        Parameters\n","        ----------\n","        data : tf.Tensor, shape = (None, n_samples, n_detectors)\n","            A batch of input waveforms, n_detectors (n_channels) should \n","            be last.\n","\n","        Returns\n","        -------\n","        tf.Tensor, shape = (None, n_samples, n_detectors)\n","            The corresponding batch of bandpassed signals.\n","        \"\"\"\n","\n","        data_ref = GeneralUtilities.broadcast_dim(data)\n","        bandpass = []\n","        for i in range(data_ref.get_shape()[-1]):\n","            x = data_ref[..., i]\n","            x = tf.cast(x, self.dtype)\n","            spec = tf.signal.rfft(x)\n","            spec *= tf.cast(self.f_response, tf.complex64)\n","            x = tf.signal.irfft(spec)\n","            x = tf.cast(x, self.dtype)\n","            x = tf.expand_dims(x, axis = -1)\n","            bandpass = x if (i == 0) else tf.concat([bandpass, x], axis = -1)\n","        \n","        bandpass /= self.norm\n","        return bandpass\n","\n","\n","    def get_config(\n","            self\n","        ) -> Mapping[str, float]:\n","        \"\"\"\n","        Function to get the configuration parameters of the object.\n","        \n","        Returns\n","        -------\n","        Mapping[str, float]\n","            Dictionary containing the configuration parameters of the object.\n","        \"\"\"\n","        config = {\n","            \"sample_rate\": self.sample_rate,\n","            \"degree\": self.degree,\n","            \"f_band\": self.f_band,\n","            \"n_samples\": self.n_samples\n","        }\n","\n","        config.update(super(BandpassLayer, self).get_config())\n","        return config"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FFnzq7yYVsW"},"source":["#### 3.3.2 Custom Regularisation/Augmentation Layers"]},{"cell_type":"markdown","metadata":{"id":"rbVxBaQeZEYD"},"source":["Regularisation layers include, in this case, channels permutation and any form of spectrogram masking."]},{"cell_type":"code","metadata":{"id":"JIa6wOmOY42k","executionInfo":{"status":"ok","timestamp":1634919625536,"user_tz":-120,"elapsed":5,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class PermuteChannel(tf.keras.layers.Layer):\n","    \"\"\"\n","    Layer that randomly permutes the channels from data to avoid overfitting \n","    in the context of G2Net.\n","    \"\"\"\n","\n","    def __init__(\n","            self, \n","            p: float = 0.1,\n","            **kwargs\n","        ) -> None:\n","        \"\"\"\n","        Function to initialise the object.\n","\n","        Parameters\n","        ----------\n","        rate : float, optional\n","            Probability of performing a permutation. The default 0.5.\n","        \"\"\"\n","    \n","        super(PermuteChannel, self).__init__(**kwargs)\n","        self.p = p\n","\n","\n","    def build(\n","            self, \n","            input_shape: Tuple[int, int]\n","        ) -> None:\n","        \"\"\"\n","        Function to build the graph of the layer. Adds trainable and non-\n","        trainable parameters if any.\n","\n","        Parameters\n","        ----------\n","        input_shape : Tuple[int, int]\n","            Shape of the input to the layer.\n","        \"\"\"\n","\n","        super(PermuteChannel, self).build(input_shape)\n","\n","\n","\n","    def call(\n","            self, \n","            data: tf.Tensor,\n","            training: bool = None\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Forward pass of the layer (requires channels to be last).\n","\n","        Parameters\n","        ----------\n","        data : tf.Tensor, shape = (None, n_samples, n_detectors)\n","            A batch of channeled inputs, n_detectors (n_channels) should be last.\n","        training : bool, optional\n","            Whether the forward pass is called in training or in prediction \n","            mode. Default is None.\n","\n","        Returns\n","        -------\n","        tf.Tensor, shape = (None, n_samples, n_detectors)\n","            The corresponding batch of channeled inputs.\n","        \"\"\"\n","\n","        x = data\n","        if training:\n","            x = tf.cond(tf.random.uniform(()) < self.p, \n","                        partial(self._permute_channels, data = x),\n","                        lambda: tf.cast(x, self.dtype))\n","        return x\n","\n","\n","    def _permute_channels(\n","            self,\n","            data: tf.Tensor\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        This funtion applies a random permutations of the channels dimension \n","        assuming channels last format.\n","\n","        Parameters\n","        ----------\n","        data : tf.Tensor\n","            Input data.\n","\n","        Returns\n","        -------\n","        tf.Tensor\n","            Permuted output data.\n","        \"\"\"\n","    \n","        x = tf.cast(data, self.dtype)\n","        perm = tf.range(data.get_shape()[-1])\n","        perm = tf.random.shuffle(perm)\n","        return tf.gather(x, perm, axis = -1)\n","\n","\n","    def get_config(\n","            self\n","        ) -> Mapping[str, float]:\n","        \"\"\"\n","        Function to get the configuration parameters of the object.\n","        \n","        Returns\n","        -------\n","        Mapping[str, float]\n","            Dictionary containing the configuration parameters of the object.\n","        \"\"\"\n","        config = {\n","            \"p\" : self.p\n","        }\n","        \n","        config.update(super(PermuteChannel, self).get_config())\n","        return config"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"8SZAG86WZlRo","executionInfo":{"status":"ok","timestamp":1634919625536,"user_tz":-120,"elapsed":5,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class _UtilitiesAug(object):\n","    \"\"\" \n","    Utilities class for augmentations/regularisations\n","    \"\"\"\n","\n","    @staticmethod\n","    def time_mask(\n","            data: tf.Tensor, \n","            param: tf.Tensor\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Apply masking to a spectrogram in the time domain. Assumes batch \n","        as the first dimension and channels as the last dimension.\n","        \n","        Parameters\n","        ----------\n","        data : tf.Tensor, shape = (None, n_freq, n_time, n_detectors)\n","            Input spectrogram.\n","        param: int\n","            Parameter of time masking indicative of width.\n","        \n","        Returns\n","        -------\n","        tf.Tensor, shape = (None, n_freq, n_time, n_detectors)\n","            Masked spectrogram.\n","        \"\"\"\n","        _, freq_max, time_max, _ = data.get_shape()\n","\n","        t0 = tf.random.uniform((), maxval = time_max - param, dtype = tf.int32)\n","\n","        indices = tf.reshape(tf.range(time_max), (1, -1))\n","        condition = tf.math.logical_and(tf.math.greater_equal(indices, t0), \n","                                       tf.math.less(indices, t0 + param))\n","\n","        mask = tf.ones([freq_max, time_max], dtype = data.dtype)\n","        mask = tf.where(condition, tf.cast(0., data.dtype), mask)\n","        mask = tf.expand_dims(tf.expand_dims(mask, axis = 0), axis = -1)\n","        return data * mask\n","\n","\n","    @staticmethod\n","    def freq_mask(\n","            data: tf.Tensor, \n","            param: tf.Tensor\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Apply masking to a spectrogram in the frequency domain. Assumes batch \n","        as the first dimension and channels as the last dimension.\n","    \n","        Parameters\n","        ----------\n","        data : tf.Tensor, shape = (None, n_freq, n_time, n_detectors)\n","            Input spectrogram.\n","        param: tf.Tensor, shape = ()\n","            Parameter of frequency masking indicative of width.\n","    \n","        Returns\n","        -------\n","        tf.Tensor, shape = (None, n_freq, n_time, n_detectors)\n","            Masked spectrogram.\n","        \"\"\"\n","    \n","        _, freq_max, time_max, _ = data.get_shape()\n","    \n","        f0 = tf.random.uniform((), maxval = time_max - param, dtype = tf.int32)\n","    \n","        indices = tf.reshape(tf.range(freq_max), (-1, 1))\n","        condition = tf.math.logical_and(tf.math.greater_equal(indices, f0), \n","                                       tf.math.less(indices, f0 + param))\n","    \n","        mask = tf.ones([freq_max, time_max], dtype = data.dtype)\n","        mask = tf.where(condition, tf.cast(0., data.dtype), mask)\n","        mask = tf.expand_dims(tf.expand_dims(mask, axis = 0), axis = -1)\n","        return data * mask"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"xi3wcjMUbnG6","executionInfo":{"status":"ok","timestamp":1634919625536,"user_tz":-120,"elapsed":5,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class SpectralMask(tf.keras.layers.Layer):\n","    \"\"\"\n","    Layer that applies spectral masks to an input spectrogram. Not usable with TPU.\n","    \"\"\"\n","\n","    def __init__(\n","            self, \n","            p: float = 0.2,\n","            n_max_mask_t: int = 2,\n","            w_mask_t: Tuple[int, int] = (5, 10),\n","            n_max_mask_f: int = 2,\n","            w_mask_f: Tuple[int, int] = (5, 10),\n","            **kwargs\n","        ) -> None:\n","        \"\"\"\n","        Function to initialise the object.\n","\n","        Parameters\n","        ----------\n","        p : float, optional\n","            Probability of applying a spectral mask. The default 0.2.\n","        n_max_mask_t : int, optional\n","            Maximum number of masks in time dimension. The default is 2.\n","        n_max_mask_f : int, optional\n","            Maximum number of masks in frequency dimension. The default is 2.\n","        \"\"\"\n","    \n","        super(SpectralMask, self).__init__(**kwargs)\n","        self.p = p\n","        self.n_max_mask_t = tf.math.maximum(n_max_mask_t, 0)\n","        self.w_min_mask_t = tf.math.maximum(w_mask_t[0], 1)\n","        self.w_max_mask_t = tf.math.maximum(w_mask_t[-1], 1)\n","        self.n_max_mask_f = tf.math.maximum(n_max_mask_f, 0)\n","        self.w_min_mask_f = tf.math.maximum(w_mask_f[0], 1)\n","        self.w_max_mask_f = tf.math.maximum(w_mask_f[-1], 1)\n","\n","\n","    def build(\n","            self, \n","            input_shape: Tuple[int, int, int]\n","        ) -> None:\n","        \"\"\"\n","        Function to build the graph of the layer. Adds trainable and non-\n","        trainable parameters if any.\n","\n","        Parameters\n","        ----------\n","        input_shape : Tuple[int, int, int]\n","            Shape of the input to the layer.\n","        \"\"\"\n","\n","        super(SpectralMask, self).build(input_shape)\n","\n","\n","\n","    def call(\n","            self, \n","            data: tf.Tensor,\n","            training: bool = None\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Forward pass of the layer (requires channels to be last).\n","\n","        Parameters\n","        ----------\n","        data : tf.Tensor, shape = (None, n_time, n_freq, n_detectors)\n","            A batch of channeled inputs, n_detectors (n_channels) should be last.\n","        training : bool, optional\n","            Whether the forward pass is called in training or in prediction \n","            mode. Default is None.\n","\n","        Returns\n","        -------\n","        tf.Tensor, shape = (None, n_time, n_freq, n_detectors)\n","            The corresponding batch of channeled inputs.\n","        \"\"\"\n","\n","        x = data\n","        if training:\n","            x = tf.cond(tf.random.uniform(()) < self.p, \n","                        partial(self._apply_all_mask, data = x),\n","                        lambda: tf.cast(x, self.dtype))\n","        return x\n","\n","\n","    def _apply_single_mask_freq(\n","            self,\n","            i: int,\n","            i_max: int,\n","            data: tf.Tensor,\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        This funtion applies a single frequency spectral mask to a spectrogram. \n","        Assumes batch as the first dimension and channels as the last dimension.\n","    \n","        Parameters\n","        ----------\n","        i : int\n","            Counter of the number of masks applied.\n","        data : tf.Tensor, shape = (None, n_freq, n_time, n_detectors)\n","            Input data.\n","    \n","        Returns\n","        -------\n","        tf.Tensor\n","            Permuted output data.\n","        \"\"\"\n","\n","        w_mask = tf.random.uniform(shape = (), minval = self.w_min_mask_f, \n","                                   maxval = self.w_max_mask_f + 1, dtype = tf.int32)\n","        x = data\n","        x = tf.cond(i < i_max, partial(_UtilitiesAug.freq_mask, param = w_mask, data = x), \n","                    partial(_UtilitiesAug.freq_mask, param = 0, data = x))\n","        return i + 1, i_max, x\n","    \n","\n","    def _apply_single_mask_time(\n","            self,\n","            i: int,\n","            i_max: int,\n","            data: tf.Tensor,\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        This funtion applies a single temporal spectral mask to a spectrogram. \n","        Assumes batch as the first dimension and channels as the last dimension.\n","    \n","        Parameters\n","        ----------\n","        i : int\n","            Counter of the number of masks applied.\n","        data : tf.Tensor, shape = (None, n_freq, n_time, n_detectors)\n","            Input data.\n","    \n","        Returns\n","        -------\n","        tf.Tensor\n","            Permuted output data.\n","        \"\"\"\n","    \n","        w_mask = tf.random.uniform(shape = (), minval = self.w_min_mask_t, \n","                                   maxval = self.w_max_mask_t + 1, dtype = tf.int32)\n","        x = data\n","        x = tf.cond(i < i_max, partial(_UtilitiesAug.time_mask, param = w_mask, data = x), \n","                    partial(_UtilitiesAug.time_mask, param = 0, data = x))\n","        return i + 1, i_max, x\n","\n","\n","    def _apply_all_mask(\n","            self,\n","            data: tf.Tensor\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        This funtion applies all spectral masks to an input spectrogram \n","        according to configuration. Assumes batch as the first dimension and \n","        channels as the last dimension.\n","\n","        Parameters\n","        ----------\n","        data : tf.Tensor, shape = (None, n_freq, n_time, n_detectors)\n","            Input data.\n","\n","        Returns\n","        -------\n","        tf.Tensor\n","            Masked output data.\n","        \"\"\"\n","    \n","        n_mask_t = tf.random.uniform(shape = (), maxval = self.n_max_mask_t + 1, \n","                                      dtype = tf.int32)\n","        n_mask_f = tf.random.uniform(shape = (), maxval = self.n_max_mask_f + 1, \n","                                      dtype = tf.int32)\n","\n","        x = data\n","        x = tf.cast(x, self.dtype)\n","        x = tf.while_loop(lambda i, i_max, inp: i < self.n_max_mask_t, \n","                  self._apply_single_mask_time, (0, n_mask_t, x),\n","                  maximum_iterations = self.n_max_mask_t)[-1]\n","        x = tf.while_loop(lambda i, i_max, inp: i < self.n_max_mask_f, \n","                  self._apply_single_mask_freq, (0, n_mask_f, x),\n","                  maximum_iterations = self.n_max_mask_f)[-1]\n","        return x\n","\n","\n","    def get_config(\n","            self\n","        ) -> Mapping[str, float]:\n","        \"\"\"\n","        Function to get the configuration parameters of the object.\n","        \n","        Returns\n","        -------\n","        Mapping[str, float]\n","            Dictionary containing the configuration parameters of the object.\n","        \"\"\"\n","        config = {\n","            \"p\" : self.p,\n","            \"n_max_mask_t\" : self.n_max_mask_t,\n","            \"w_mask_t\" : self.w_mask_t,\n","            \"n_max_mask_f\" : self.n_max_mask_t,\n","            \"w_mask_f\" : self.w_mask_t\n","        }\n","        \n","        config.update(super(SpectralMask, self).get_config())\n","        return config"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"2pCYRmgMb1GY","executionInfo":{"status":"ok","timestamp":1634919625536,"user_tz":-120,"elapsed":4,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class TimeMask(tf.keras.layers.Layer):\n","    \"\"\"\n","    Layer that applies a single time mask to an input spectrogram.\n","    \"\"\"\n","\n","    def __init__(\n","            self, \n","            p: float = 0.2,\n","            w_mask: Tuple[int, int] = (5, 10),\n","            **kwargs\n","        ) -> None:\n","        \"\"\"\n","        Function to initialise the object.\n","\n","        Parameters\n","        ----------\n","        p : float, optional\n","            Probability of applying a spectral mask. The default 0.2.\n","        w_mask : Tuple[int, int], optional\n","            Minimum and maximum width of the mask in pixels. The default is (5, 10).\n","        \"\"\"\n","    \n","        super(TimeMask, self).__init__(**kwargs)\n","        self.p = tf.constant(p, self.dtype)\n","        self.w_min_mask = tf.math.maximum(w_mask[0], 1)\n","        self.w_max_mask = tf.math.maximum(w_mask[-1], 1)\n","\n","\n","    def build(\n","            self, \n","            input_shape: Tuple[int, int, int]\n","        ) -> None:\n","        \"\"\"\n","        Function to build the graph of the layer. Adds trainable and non-\n","        trainable parameters if any.\n","\n","        Parameters\n","        ----------\n","        input_shape : Tuple[int, int, int]\n","            Shape of the input to the layer.\n","        \"\"\"\n","\n","        super(TimeMask, self).build(input_shape)\n","\n","\n","\n","    def call(\n","            self, \n","            data: tf.Tensor,\n","            training: bool = None\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Forward pass of the layer (requires channels to be last).\n","\n","        Parameters\n","        ----------\n","        data : tf.Tensor, shape = (None, n_freq, n_time, n_detectors)\n","            A batch of channeled inputs, n_detectors (n_channels) should be last.\n","        training : bool, optional\n","            Whether the forward pass is called in training or in prediction \n","            mode. Default is None.\n","\n","        Returns\n","        -------\n","        tf.Tensor, shape = (None, n_freq, n_time, n_detectors)\n","            The corresponding batch of channeled inputs.\n","        \"\"\"\n","\n","        x = data\n","        if training:\n","            w_mask = tf.random.uniform(shape = (), minval = self.w_min_mask, \n","                                       maxval = self.w_max_mask + 1, dtype = tf.int32)\n","            x = tf.cast(x, self.dtype)\n","            x = tf.cond(tf.random.uniform(()) < self.p,\n","                        partial(_UtilitiesAug.time_mask, param = w_mask, data = x),\n","                        partial(_UtilitiesAug.time_mask, \n","                        param = tf.constant(0, tf.int32), data = x))\n","        return x\n","\n","\n","    def get_config(\n","            self\n","        ) -> Mapping[str, float]:\n","        \"\"\"\n","        Function to get the configuration parameters of the object.\n","        \n","        Returns\n","        -------\n","        Mapping[str, float]\n","            Dictionary containing the configuration parameters of the object.\n","        \"\"\"\n","        config = {\n","            \"p\" : self.p,\n","            \"w_min_mask\" : self.w_min_mask,\n","            \"w_max_mask\" : self.w_max_mask\n","        }\n","        \n","        config.update(super(TimeMask, self).get_config())\n","        return config"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ec9U1RKFb3-1","executionInfo":{"status":"ok","timestamp":1634919625537,"user_tz":-120,"elapsed":5,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class FreqMask(tf.keras.layers.Layer):\n","    \"\"\"\n","    Layer that applies a single frequency mask to an input spectrogram.\n","    \"\"\"\n","\n","    def __init__(\n","            self, \n","            p: float = 0.2,\n","            w_mask: Tuple[int, int] = (5, 10),\n","            **kwargs\n","        ) -> None:\n","        \"\"\"\n","        Function to initialise the object.\n","\n","        Parameters\n","        ----------\n","        p : float, optional\n","            Probability of applying a spectral mask. The default 0.2.\n","        w_mask : Tuple[int, int], optional\n","            Minimum and maximum width of the mask in pixels. The default is (5, 10).\n","        \"\"\"\n","    \n","        super(FreqMask, self).__init__(**kwargs)\n","        self.p = p\n","        self.w_min_mask = tf.math.maximum(w_mask[0], 1)\n","        self.w_max_mask = tf.math.maximum(w_mask[-1], 1)\n","\n","\n","    def build(\n","            self, \n","            input_shape: Tuple[int, int, int]\n","        ) -> None:\n","        \"\"\"\n","        Function to build the graph of the layer. Adds trainable and non-\n","        trainable parameters if any.\n","\n","        Parameters\n","        ----------\n","        input_shape : Tuple[int, int, int]\n","            Shape of the input to the layer.\n","        \"\"\"\n","\n","        super(FreqMask, self).build(input_shape)\n","\n","\n","    def call(\n","            self, \n","            data: tf.Tensor,\n","            training: bool = None\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Forward pass of the layer (requires channels to be last).\n","\n","        Parameters\n","        ----------\n","        data : tf.Tensor, shape = (None, n_freq, n_time, n_detectors)\n","            A batch of channeled inputs, n_detectors (n_channels) should be last.\n","        training : bool, optional\n","            Whether the forward pass is called in training or in prediction \n","            mode. Default is None.\n","\n","        Returns\n","        -------\n","        tf.Tensor, shape = (None, n_freq, n_time, n_detectors)\n","            The corresponding batch of channeled inputs.\n","        \"\"\"\n","\n","        x = data\n","        if training:\n","            w_mask = tf.random.uniform((), minval = self.w_min_mask, \n","                                       maxval = self.w_max_mask + 1, dtype = tf.int32)\n","            x = tf.cast(x, self.dtype)\n","            x = tf.cond(tf.random.uniform(()) < self.p,\n","                        partial(_UtilitiesAug.freq_mask, param = w_mask, data = x),\n","                        partial(_UtilitiesAug.freq_mask, param = tf.constant(0, tf.int32), \n","                                data = x))\n","        return x\n","\n","\n","    def get_config(\n","            self\n","        ) -> Mapping[str, float]:\n","        \"\"\"\n","        Function to get the configuration parameters of the object.\n","        \n","        Returns\n","        -------\n","        Mapping[str, float]\n","            Dictionary containing the configuration parameters of the object.\n","        \"\"\"\n","        config = {\n","            \"p\" : self.p,\n","            \"w_min_mask\" : self.w_min_mask,\n","            \"w_max_mask\" : self.w_max_mask\n","        }\n","        \n","        config.update(super(FreqMask, self).get_config())\n","        return config"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MWgL5ytgb-ey"},"source":["#### 3.3.3 Custom Spectrogram Layer"]},{"cell_type":"markdown","metadata":{"id":"GbHc1gJB6qZb"},"source":["The time-domain to time-frequency representation conversor is, perhaps, one of the most important parts of the model. Being a custom layer prepared for the needs of the problem, it changes the paradigm from time-series to a computer vision problem. The CQT1992v2 layer chosen for this purpose is implemented in the following cells."]},{"cell_type":"code","metadata":{"id":"4L_r172PcEWf","executionInfo":{"status":"ok","timestamp":1634919625537,"user_tz":-120,"elapsed":5,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class _UtilitiesCQT(object):\n","    \"\"\"\n","    Class with local auxiliary function for spectrograms.\n","    \"\"\"\n","\n","    @staticmethod\n","    def create_cqt_kernels(\n","            q: float,\n","            sample_rate: float,\n","            f_band: Tuple[float, float] = (0., None),\n","            n_bins: int = 84,\n","            bins_per_octave: int = 12,\n","            norm: int = 1,\n","            window: Union[str, Tuple[float, str]] = \"hann\"\n","        ) -> Tuple[np.ndarray, int, np.ndarray, np.ndarray]:\n","        \"\"\"\n","        Function to automatically create CQT kernels in time domain.\n","        \n","        Parameters\n","        ----------\n","        q : float\n","            Q parameter.\n","        sample_rate : float\n","            The sampling rate for the input time series. It is used to \n","            calculate the correct \"f_min\" and \"f_max\". \n","        f_band : Tuple[float, float], optional\n","            The frequency for the lowest (f_min) and highest (f_max) CQT bin [Hz]. \n","            The default is (0., None). Since the default highest CQT bin frequency \n","            is None, it will be inferred from n_bins and bins_per_octave. \n","            If provided, n_bins will be ignored. \n","        n_bins : int, optional\n","            The total numbers of CQT bins. Will be ignored if \"f_max\" is not None. \n","            The default is 84. \n","        bins_per_octave : int, optional\n","            Number of bins per octave. The default is 12.\n","        norm : int, optional\n","            Normalization for the CQT kernels. 1 means L1 normalization and 2 \n","            means L2 normalization. The default is 1, which is same as the \n","            normalization used in librosa.\n","        window : Union[str, Tuple[float, str]], optional\n","            The windowing function for CQT. If it is a string, It uses \n","            \"scipy.signal.get_window\". If it is a tuple, only the gaussian \n","            window wanrantees constant Q factor. The default is \"hann\".\n","\n","        Raises\n","        ------\n","        ValueError\n","            If maximum bins frequency is greater than the Nyquist frequency.\n","\n","        Returns\n","        -------\n","        Tuple[np.ndarray, int, np.ndarray, np.ndarray]\n","            CQT kernels, length of the frequency bins and associated \n","            frequencies.\n","        \"\"\"\n","\n","        f_min, f_max = f_band[0], f_band[-1]\n","\n","        len_min = np.ceil(q * sample_rate / f_min)\n","        fft_len = 2 ** np.int(np.ceil(np.log2(len_min)))\n","    \n","        if (f_max is not None) and (n_bins is None):\n","            n_bins = np.ceil(bins_per_octave * np.log2(f_max / f_min))\n","            freqs = f_min * 2. ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n","        elif (f_max is None) and (n_bins is not None):\n","            freqs = f_min * 2. ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n","        else:\n","            warnings.warn(\"If f_max is given, n_bins will be ignored\", SyntaxWarning)\n","            n_bins = np.ceil(bins_per_octave * np.log2(f_max / f_min))\n","            freqs = f_min * 2. ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n","\n","        f_nyq = sample_rate / 2.\n","        if np.max(freqs) > f_nyq:\n","            raise ValueError(f\"The top bin {np.max(freqs)} Hz has exceeded \\\n","                             the Nyquist frequency, please reduce `n_bins`\")\n","\n","        kernel = np.zeros((np.int(n_bins), np.int(fft_len)), dtype = np.complex64)\n","    \n","        lengths = np.ceil(q * sample_rate / freqs)\n","        for k in range(np.int(n_bins)):\n","            freq = freqs[k]\n","            l = np.ceil(q * sample_rate / freq)\n","\n","            if l % 2 == 1:\n","                start = np.int(np.ceil(fft_len / 2. - l / 2.)) - 1\n","            else:\n","                start = np.int(np.ceil(fft_len / 2. - l / 2.))\n","    \n","            sig = signal.get_window(window, np.int(l), fftbins = True)\n","            sig = sig * np.exp(np.r_[-l // 2:l // 2] * 1j * 2 * np.pi * \\\n","                               freq / sample_rate) / l\n","            \n","            if norm:\n","                kernel[k, start:start + np.int(l)] = sig / np.linalg.norm(sig, norm)\n","            else:\n","                kernel[k, start:start + np.int(l)] = sig\n","\n","        return kernel, fft_len, lengths, freqs"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEIVACSTc1_M","executionInfo":{"status":"ok","timestamp":1634919625855,"user_tz":-120,"elapsed":322,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["class CQTLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    Constant-Q Transform keras layer. Based on the nnaudio implementation \n","    and extended to bound the output spectrogram to the input of an image-based\n","    model.\n","    \n","    See \"K. W. Cheuk, H. Anderson, K. Agres and D. Herremans, nnAudio: An \n","    on-the-Fly GPU Audio to Spectrogram Conversion Toolbox Using 1D \n","    Convolutional Neural Networks, in IEEE Access, vol. 8, pp. 161981-162003, \n","    2020, doi: 10.1109/ACCESS.2020.3019084. \n","    \n","    https://github.com/KinWaiCheuk/nnAudio\n","    \"\"\"\n","\n","    def __init__(\n","            self, \n","            sample_rate: float = 2048., \n","            hop_length: int = 32, \n","            n_bins = 84,\n","            bins_per_octave: int = 12,\n","            f_band: Tuple[float, float] = (0., None),\n","            norm: int = 1,\n","            filter_scale: int = 1,\n","            window: str = \"hann\",\n","            center: bool = True, \n","            pad_mode: str = \"reflect\",\n","            norm_type: str = \"librosa\",\n","            image_out: bool = True,\n","            perc_range: float = 0.05,\n","            minmax_init: Tuple[float, float] = (0, -1e7),\n","            tpu: bool = False,\n","            **kwargs\n","        ) -> None:\n","        \"\"\"\n","        Function to initialise the object.\n","\n","        Parameters\n","        ----------\n","        sample_rate : float, optional\n","            The sampling rate for the input time series [Hz]. It is used to \n","            calculate the correct \"f_min\" and \"f_max\". The default is 2048.\n","        hop_length : int, optional\n","            The hop (or stride) size. The default is 512.\n","        n_bins : int, optional\n","            The total numbers of CQT bins. Will be ignored if \"f_max\" is not None. \n","            The default is 32. \n","        bins_per_octave : int, optional\n","            Number of bins per octave. The default is 12.\n","        f_band : Tuple[float, float], optional\n","            The frequency for the lowest (f_min) and highest (f_max) CQT bin [Hz]. \n","            The default is (0., None). Since the default highest CQT bin frequency \n","            is None, it will be inferred from n_bins and bins_per_octave. \n","            If provided, n_bins will be ignored. \n","        norm : int, optional\n","            Normalization for the CQT kernels. 1 means L1 normalization and 2 \n","            means L2 normalization. The default is 1, which is same as the \n","            normalization used in librosa.\n","        filter_scale : int, optional\n","            Filter scale factor. Values of filter_scale smaller than 1 can be \n","            used to improve the time resolution at the cost of degrading the \n","            frequency resolution. The default is 1.\n","        window : Union[str, Tuple[str, float]], optional\n","            The windowing function for CQT. If it is a string, It uses \n","            \"scipy.signal.get_window\". If it is a tuple, only the gaussian \n","            window wanrantees constant Q factor. The default is \"hann\".\n","        center : bool\n","            Putting the CQT keneral at the center of the time-step or not.\n","            The default is True.\n","        pad_mode : str, optional\n","            The padding method. The default is \"reflect\".\n","            The possible options are:\n","                - \"constant\"\n","                - \"reflect\"\n","        norm_type : str, optional\n","            Type of the normalization. The default is \"librosa\". \n","            The possible options are: \n","                - \"librosa\" : the output fits the librosa one.\n","                - \"convolutional\" : the output conserves the convolutional \n","                  inequalities of the wavelet transform.\n","                - \"wrap\" : wraps positive and negative frequencies into \n","                  positive frequencies. \n","        image_out : bool, optional\n","            Whether to return a spectrogram scaled to the 0-255 range with \n","            current minimum and maximum. Default is True.\n","        perc_range : float, optional\n","            Extra range to apply to tracked spectrogram output maximum to \n","            leave a safe margin for non-seen examples. The default is 0.05.\n","        minmax_init : Tuple[float, float], optional\n","            Initial values for tracking minimum and maximum spectrogram outputs.\n","            The default is (0., -1e7).\n","        tpu : str, optional\n","            Whether this layer is to be applied in TPU or not. The default is False. \n","        \"\"\"\n","    \n","        super(CQTLayer, self).__init__(**kwargs)\n","        self.sample_rate = sample_rate\n","        self.n_bins = n_bins\n","        self.hop_length = hop_length\n","        self.bins_per_octave = bins_per_octave\n","        self.f_band = f_band\n","        self.norm = norm\n","        self.filter_scale = filter_scale\n","        self.window = window\n","        self.center = center\n","        self.pad_mode = pad_mode\n","        self.norm_type = norm_type\n","        self.perc_range = perc_range\n","        self.image_out = image_out\n","        self.minmax_init = minmax_init\n","        self.tpu = tpu\n","\n","        q = np.float(filter_scale) / (2. ** (1. / bins_per_octave) - 1.)\n","        cqt_kernels, kernel_width, lengths, _ = _UtilitiesCQT.create_cqt_kernels(\n","            q, sample_rate, f_band, n_bins, bins_per_octave, norm, window)\n","\n","        cqt_kernels_real = np.swapaxes(cqt_kernels.real[:, np.newaxis, :], 0, -1)\n","        cqt_kernels_imag = np.swapaxes(cqt_kernels.imag[:, np.newaxis, :], 0, -1)\n","        \n","        self.cqt_kernels_real = tf.Variable(initial_value = cqt_kernels_real, \n","                                            trainable = self.trainable,\n","                                            name = self.name + \"/real_kernels\", \n","                                            dtype = self.dtype)\n","        self.cqt_kernels_imag = tf.Variable(initial_value = cqt_kernels_imag, \n","                                            trainable = self.trainable,\n","                                            name = self.name + \"/imag_kernels\",\n","                                            dtype = self.dtype)\n","\n","        padding = tf.constant([[0, 0], [kernel_width // 2, kernel_width // 2],\n","                               [0, 0]])\n","    \n","        self.padding_fn = lambda x: x\n","        self.padding_conv = \"VALID\"\n","        if center:\n","            if self.tpu:\n","                self.padding_conv = \"SAME\"\n","                warnings.warn(\"Using TPU, changing to compatible version\", \n","                              SyntaxWarning)\n","            else:\n","                if pad_mode == \"constant\":\n","                    self.padding_fn = lambda x: tf.pad(x, padding, mode = \"CONSTANT\")\n","                elif pad_mode == \"reflect\":\n","                    self.padding_fn = lambda x: tf.pad(x, padding, mode = \"REFLECT\")\n","                else:\n","                    warnings.warn(\"Padding method not recognised, applying no padding\", \n","                                  SyntaxWarning)\n","                \n","        self.norm_factor = 1.\n","        lengths = tf.constant(lengths, dtype = self.cqt_kernels_real.dtype)\n","        if norm_type == \"librosa\":\n","            self.norm_factor = tf.math.sqrt(lengths)\n","        elif norm_type == \"convolutional\":\n","            self.norm_factor = 1.\n","        elif norm_type == \"wrap\":\n","            self.norm_factor = 2.\n","        else:\n","            warnings.warn(\"Normalization method not recognised, \\\n","                          applying convolutional normalization\", \n","                          SyntaxWarning)\n","                \n","        self.image_out = image_out\n","        self.max = tf.Variable(initial_value = minmax_init[-1], \n","                               name = self.name + \"/max\", \n","                               dtype = self.dtype)\n","        self.min = tf.Variable(initial_value = minmax_init[0], \n","                               name = self.name + \"/min\", \n","                               dtype = self.dtype)\n","\n","\n","    def build(\n","            self, \n","            input_shape: Tuple[int, int, int]\n","        ) -> None:\n","        \"\"\"\n","        Function to build the graph of the layer. Adds trainable and non-\n","        trainable parameters.\n","\n","        Parameters\n","        ----------\n","        input_shape : Tuple[int, int]\n","            Shape of the input to the layer.\n","        \"\"\"\n","\n","        if self.trainable:\n","            self.trainable_weights.append(self.cqt_kernels_real)\n","            self.trainable_weights.append(self.cqt_kernels_imag)\n","        else:\n","            self.non_trainable_weights.append(self.cqt_kernels_real)\n","            self.non_trainable_weights.append(self.cqt_kernels_imag)\n","\n","        self.non_trainable_weights.append(self.max)\n","        self.non_trainable_weights.append(self.min)\n","\n","        super(CQTLayer, self).build(input_shape)\n","\n","\n","    def call(\n","            self, \n","            data: tf.Tensor,\n","            training: bool = None\n","        ) -> tf.Tensor:\n","        \"\"\"\n","        Forward pass of the layer.\n","\n","        Parameters\n","        ----------\n","        data : tf.Tensor, shape = (None, n_samples, n_detectors)\n","            A batch of input mono waveforms, n_detectors should be last\n","        training : bool, optional\n","            Whether the forward pass is called in training or in prediction \n","            mode. Default is None.\n","\n","        Returns\n","        -------\n","        tf.Tensor, shape = (None, n_time, n_freq, n_detectors)\n","            The corresponding batch of constant Q transforms.\n","        \"\"\"\n","\n","        CQT = []\n","        for i in range(data.get_shape()[-1]):\n","            x = data[..., i]\n","            x = GeneralUtilities.broadcast_dim(x)\n","            x = tf.cast(x, self.dtype)\n","            if not self.tpu:\n","                x = self.padding_fn(x)\n","            x_real = tf.nn.conv1d(x, self.cqt_kernels_real, \n","                                  stride = self.hop_length, \n","                                  padding = self.padding_conv)\n","            x_imag = -tf.nn.conv1d(x, self.cqt_kernels_imag, \n","                                   stride = self.hop_length, \n","                                   padding = self.padding_conv)\n","            x_real *= self.norm_factor\n","            x_imag *= self.norm_factor\n","            x = tf.pow(x_real, 2) + tf.pow(x_imag, 2)\n","            if self.trainable:\n","                x += 1e-8\n","            x = tf.math.sqrt(x)\n","            x = tf.transpose(x, [0, 2, 1])\n","            x = tf.expand_dims(x, axis = -1)\n","            CQT = x if (i == 0) else tf.concat([CQT, x], axis = -1)\n","            \n","        if self.image_out:\n","            if training:\n","                max_batch = tf.stop_gradient(tf.reduce_max(CQT))\n","                max_val = tf.stop_gradient(tf.math.maximum(self.max, max_batch))\n","                min_batch = tf.stop_gradient(tf.reduce_min(CQT))\n","                min_val = tf.stop_gradient(tf.math.minimum(self.min, min_batch))\n","        \n","                self.max.assign(max_val)\n","                self.min.assign(min_val)\n","\n","            r_minmax = tf.stop_gradient(self.max - self.min)\n","            min_val = tf.stop_gradient(self.min)\n","            max_val = tf.stop_gradient(self.max + self.perc_range * r_minmax)\n","            CQT = (CQT - min_val)/(max_val - min_val)\n","            \n","            CQT = tf.clip_by_value(CQT, clip_value_min = 0., clip_value_max = 1.)\n","\n","        return CQT\n","\n","\n","    def get_config(\n","            self\n","        ) -> Mapping[str, float]:\n","        \"\"\"\n","        Function to get the configuration parameters of the object.\n","        \n","        Returns\n","        -------\n","        Mapping[str, float]\n","            Dictionary containing the configuration parameters of the object.\n","        \"\"\"\n","        config = {\n","            \"sample_rate\": self.sample_rate,\n","            \"n_bins\": self.n_bins,\n","            \"hop_length\": self.hop_length,\n","            \"bins_per_octave\": self.bins_per_octave,\n","            \"f_band\": self.f_band,\n","            \"norm\": self.norm,\n","            \"filter_scale\": self.filter_scale,\n","            \"window\": self.window,\n","            \"center\": self.center,\n","            \"pad_mode\": self.pad_mode,\n","            \"norm_type\": self.norm_type,\n","            \"perc_range\": self.perc_range,\n","            \"image_out\": self.image_out,\n","            \"minmax_init\": self.minmax_init,\n","            \"tpu\": self.tpu\n","        }\n","        \n","        config.update(super(CQTLayer, self).get_config())\n","        return config"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVcqHwHV70h5"},"source":["#### 3.3.4 The Model"]},{"cell_type":"markdown","metadata":{"id":"XyijjBsj8BXP"},"source":["Run the following piece of code to define a function that builds and compiles the model. Pay attention to the fact that the model is compiled in the scope of the function to avoid problems with TPU."]},{"cell_type":"code","metadata":{"id":"-dbv15kpRtOF","executionInfo":{"status":"ok","timestamp":1634919625856,"user_tz":-120,"elapsed":2,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["def build_model(\n","            input_shape: Tuple[int, int],\n","            window_shape: float = 0.2,\n","            trainable_window: bool = False,\n","            sample_rate: float = 2048., \n","            degree_filt: int = 8,\n","            f_band_filt: Tuple[float, float] = (20., 500.),\n","            trainable_filt: bool = False,\n","            hop_length: int = 64,\n","            f_band_spec: Tuple[float, float] = (20., 500.),\n","            bins_per_octave: int = 12,\n","            window_cqt: str = \"hann\",\n","            perc_range: float = 0.01, \n","            trainable_cqt: bool = False,\n","            resize_shape: Tuple[int, int] = (128, 128),\n","            p_perm: float = 0.1,\n","            p_mask: float = 0.1,\n","            n_max_mask_t: int = 2,\n","            w_mask_t: Tuple[int, int] = (12, 25),\n","            n_max_mask_f: int = 2,\n","            w_mask_f: Tuple[int, int] = (12, 25),\n","            dtype: type = tf.float32,\n","            strategy: str = \"GPU\",\n","            effnet_id: str = \"efficientnetv2-b0\",\n","            weights: str = \"imagenet\",\n","            learning_rate: float = 0.001\n","        ) -> tf.keras.Model:\n","        \"\"\"\n","        Function to build and compile the model.\n","        \n","        Parameters\n","        ----------\n","        input_shape : Tuple[int, int], \n","            Shape of the input to the model without accounting for batch size.\n","        window_shape : float, optional\n","            Shape parameter of the Tukey temporal window. The default is 0.2.\n","        trainable_window : bool, optional\n","            Whether the Tukey temporal window should be trained or not. \n","            The default is False.\n","        sample_rate : float, optional\n","            The sampling rate for the input time series. The default is 2048.\n","        degree_filt : int, optional\n","            Degree of the bandpass filter. The default is 8.\n","        f_band_filt : Tuple[float, float], optional\n","            The frequency band for the bandpass filter [Hz]. The default \n","            is (20, 500).\n","        trainable_filt : bool, optional\n","            Whether the bandpass filter should be trained or not. \n","            The default is False.\n","        hop_length : int, optional\n","            The hop (or stride) size for the CQT layer. The default is 512.\n","        f_band_spec : Tuple[float, float], optional\n","            The frequency for the lowest (f_min) and highest (f_max) CQT bins [Hz]. \n","            The default is (20, 500).\n","        bins_per_octave : int, optional\n","            Number of bins per octave for the CQT layer. The default is 12.\n","        window_cqt : str, optional\n","            The windowing function for CQT. The default is \"hann\".\n","        perc_range : float, optional\n","            Extra range to apply to tracked spectrogram output maximum to \n","            leave a safe margin for non-seen examples. The default is 0.05.\n","        trainable_cqt : bool, optional\n","            Whether the cqt layer should be trained or not. If transfer learning \n","            is applied, the recommendation is to freeze this layer during the \n","            first epochs and activate its training afterwards. The default is False.\n","        resize_shape : Tuple[int, int], optional\n","            Spectrogram resize shape without including batch size and channels. \n","            The default is (128, 128).\n","        p_perm : float, optional\n","            Probability of performing a channel permutation for regularisation.\n","            The default is 0.1.\n","        p_mask : foat, optional\n","            Probability of performing spectral mask for regularisation. The \n","            default is 0.1.\n","        n_max_mask_t : int, optional\n","            Maximum number of masks in time dimension. The default is 2.\n","        w_mask_t : Tuple[int, int], optional\n","            Minimum and maximum width of masking bands in the time dimension. \n","            The default is (12, 25).\n","        n_max_mask_f : int, optional\n","            Maximum number of masks in frequency dimension. The default is 2.\n","        w_mask_f : Tuple[int, int], optional\n","            Minimum and maximum width of masking bands in the frequency dimension. \n","            The default is (12, 25).\n","        dtype : type, optional\n","            Data type of the model layer parameters. The default is tf.float32.\n","        strategy : str, optional\n","            In use strategy. It is mainly used to switch to layers compatible \n","            with XLA when using TPU. The default is \"GPU\".\n","            Available options are:\n","                - \"TPU\"\n","                - \"GPU\"\n","                - \"CPU\"\n","        effnet_id : str, optional\n","            Id of the efficientnet backend model to use. The default is \"efficientnetv2-b2\".\n","            Available options are:\n","                - \"efficientnetv2-s\"\n","                - \"efficientnetv2-m\"\n","                - \"efficientnetv2-l\"\n","                - \"efficientnetv2-xl\"\n","                - \"efficientnetv2-b0\"\n","                - \"efficientnetv2-b1\"\n","                - \"efficientnetv2-b2\"\n","                - \"efficientnetv2-b3\"\n","                - \"efficientnet-b0\"\n","                - \"efficientnet-b1\"\n","                - \"efficientnet-b2\"\n","                - \"efficientnet-b3\"\n","                - \"efficientnet-b4\"\n","                - \"efficientnet-b5\"\n","                - \"efficientnet-b6\"\n","                - \"efficientnet-b7\"\n","                - \"efficientnet-b8\"\n","                - \"efficientnet-l2\"\n","        weights : str, optional\n","            Whether to use weights from pre-trained models or not. The default \n","            is \"imagenet\". Available options are:\n","                - \"imagenet\"\n","                - \"imagenet21k\"\n","                - \"imagenet21k-ft1k\"\n","                - \"jft\"\n","        learning_rate : float, optional\n","            Learning rate for the optimizer. The default is 0.001.\n","        \"\"\"\n","\n","        inp = Input(shape = input_shape, dtype = dtype, name = \"input\")\n","\n","        if strategy == \"TPU\":\n","            window = WindowingLayer(window = (\"tukey\", window_shape),\n","                                    window_len = input_shape[0],\n","                                    trainable = trainable_window, name = \"window\")\n","        else:\n","            window = TukeyWinLayer(initial_alpha = window_shape, \n","                                   trainable = trainable_window, name = \"window\")\n","\n","        bandpass = BandpassLayer(sample_rate = sample_rate, degree = degree_filt, \n","                                      f_band = f_band_filt, n_samples = input_shape[0], \n","                                      trainable = trainable_filt, name = \"bandpass\")\n","        \n","        tpu = True if strategy == \"TPU\" else False\n","        cqt = CQTLayer(sample_rate = sample_rate, hop_length = hop_length, \n","                       f_band = f_band_spec, bins_per_octave = bins_per_octave,\n","                       window = window_cqt, trainable = trainable_cqt, \n","                       perc_range = perc_range, tpu = tpu, name = \"cqt\")\n","\n","        resize = Resizing(resize_shape[0], resize_shape[1], name = \"resize\")\n","        permute = PermuteChannel(p = p_perm, name = \"permute\")\n","        \n","        if strategy == \"TPU\":\n","            mask_t = TimeMask(p = p_mask, w_mask = w_mask_t, name = \"mask_t\")\n","            mask_f = FreqMask(p = p_mask, w_mask = w_mask_f, name = \"mask_f\")\n","        else:\n","            mask = SpectralMask(p = p_mask, n_max_mask_t = n_max_mask_t,\n","                                w_mask_t = w_mask_t, n_max_mask_f = n_max_mask_f,\n","                                w_mask_f = w_mask_f, name = \"mask\")\n","\n","        flatten = Flatten(name = \"flatten\")\n","        dense = Dense(units = 1, activation = \"sigmoid\", name = \"dense\")\n","\n","        effnet_config = copy.deepcopy(hparams.base_config)\n","        effnet_config.override(effnetv2_configs.get_model_config(effnet_id))\n","        if strategy == \"TPU\" and not effnet_config.model.bn_type:\n","            effnet_config.model.bn_type = \"tpu_bn\"\n","\n","        effnet = effnetv2_model.get_model(model_name = effnet_id,\n","            model_config = effnet_config.model, include_top = False, \n","            weights = weights)\n","\n","        x = inp\n","\n","        y = window(x)\n","        y = bandpass(y)\n","        y = cqt(y)\n","        y = resize(y)\n","        y = permute(y)\n","\n","        if strategy == \"TPU\":\n","            for _ in range(n_max_mask_t):\n","                y = mask_t(y)\n","            for _ in range(n_max_mask_f):\n","                y = mask_f(y)\n","        else:\n","            y = mask(y)\n","\n","        y = effnet(y)\n","        y = flatten(y)\n","        y = dense(y)\n","        model = tf.keras.Model(inputs = [x], outputs = [y])\n","        \n","        optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n","        # optimizer = AdaBeliefOptimizer(learning_rate = learning_rate, \n","        #                                amsgrad = False, print_change_log = False)\n","        loss = tf.keras.losses.BinaryCrossentropy()\n","        metric = tf.keras.metrics.AUC()\n","        model.compile(optimizer = optimizer, loss = loss, metrics = [metric])\n","        \n","        return model"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bx0QYH7PiDhj"},"source":["### 3.4 Model Training"]},{"cell_type":"markdown","metadata":{"id":"8lnZ7sACi4FU"},"source":["At this point, it only remains to prepare the datasets with the classes defined above before proceding with training. Of note is that a small proportion of the training dataset is stripped and used for validation purposes."]},{"cell_type":"code","metadata":{"id":"A0bz_4UVRtOG","executionInfo":{"status":"ok","timestamp":1634919633531,"user_tz":-120,"elapsed":7677,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["# Prepare original dataframes \n","sub_file = OV_PATH.joinpath(\"g2net-float32-test\", \"sample_submission.csv\")\n","train_labels_file = OV_PATH.joinpath(\"g2net-float32-train\", \"training_labels.csv\")\n","\n","train_df_ori = pd.read_csv(train_labels_file)\n","test_df_ori = pd.read_csv(sub_file)\n","\n","train_df = pd.DataFrame([Path(x).stem for x in tf.io.gfile.glob(TRAIN_PATH + \"/*.tfrec\")], columns = [\"id\"])\n","train_df = train_df.sample(frac = 1, random_state = Config.SEED_SPLIT).reset_index(drop = True)\n","test_df = pd.DataFrame([Path(x).stem for x in tf.io.gfile.glob(TEST_PATH + \"/*.tfrec\")], columns = [\"id\"])\n","\n","n_split = np.int32(train_df.shape[0] * Config.SPLIT)\n","training_df = train_df.loc[:n_split - 1, :]\n","validation_df = train_df.loc[n_split:, :]\n","        \n","training_gen = DatasetGeneratorTF(training_df, TRAIN_PATH, batch_size = Config.BATCH_SIZE, dtype = dtype)\n","validation_gen = DatasetGeneratorTF(validation_df, TRAIN_PATH, batch_size = Config.BATCH_SIZE, dtype = dtype)\n","test_gen = DatasetGeneratorTF(test_df, TEST_PATH, batch_size = Config.BATCH_SIZE_TEST, dtype = dtype)\n","    \n","training_ds = training_gen.get_dataset(buffer_size = 2048)\n","validation_ds = validation_gen.get_dataset(buffer_size = 2048)\n","test_ds = test_gen.get_dataset(shuffle = False, repeat = False, target = False)\n","\n","# Estimate number of steps per train, validation and test sets\n","ns_training = np.int32(train_df_ori.shape[0] * Config.SPLIT)\n","ns_validation = train_df_ori.shape[0] - ns_training\n","ns_test = test_df_ori.shape[0]\n","spe_training = np.int32(np.ceil(ns_training / Config.BATCH_SIZE))\n","spe_validation = np.int32(np.ceil(ns_validation / Config.BATCH_SIZE))\n","spe_test = np.int32(np.ceil(ns_test / Config.BATCH_SIZE_TEST))\n","\n","test_ds_id = test_ds.map(lambda data, identity: tf.strings.unicode_encode(\n","    identity, \"UTF-8\"))\n","test_ds_id = test_ds_id.unbatch()\n","test_ids = next(iter(test_ds_id.batch(test_df_ori.shape[0]))).numpy().astype(\"U\")\n","test_ds = test_ds.map(lambda data, identity: data)"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W_wXjaNHjNJe"},"source":["Now we are ready to build the model within the strategy scope (TPU), preload model weights from local storage (weights other than those provided by AutoML) and train it."]},{"cell_type":"code","metadata":{"id":"2l7TcMlhRtOH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634919657127,"user_tz":-120,"elapsed":23605,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}},"outputId":"54433ede-6db9-410f-fed5-dd62159ebe9a"},"source":["# Create model, compile and display summary within the scope of the \n","# distribution strategy\n","tf.keras.backend.clear_session()\n","with strategy.scope():\n","    model = build_model(input_shape = (Config.N_SAMPLES, Config.N_DETECT), \n","                        window_shape = Config.TUKEY_SHAPE,\n","                        trainable_window = Config.TRAINABLE_TUKEY, \n","                        sample_rate = Config.SAMPLE_RATE,\n","                        degree_filt = Config.DEGREE_FILT, \n","                        f_band_filt = Config.F_BAND_FILT,\n","                        trainable_filt = Config.TRAINABLE_FILT, \n","                        hop_length = Config.HOP_LENGTH,\n","                        f_band_spec = Config.F_BAND_SPEC, \n","                        bins_per_octave = Config.BINS_PER_OCTAVE,\n","                        window_cqt = Config.WINDOW_CQT, \n","                        resize_shape = (Config.IMAGE_SIZE, Config.IMAGE_SIZE),\n","                        p_perm = Config.P_PERM, p_mask = Config.P_MASK,\n","                        n_max_mask_t = Config.N_MAX_MASK, \n","                        w_mask_t = Config.W_MASK, \n","                        n_max_mask_f = Config.N_MAX_MASK,\n","                        w_mask_f = Config.W_MASK, \n","                        strategy = device, weights = CKT_PATH, \n","                        effnet_id = Config.MODEL_ID, \n","                        learning_rate = Config.LEARNING_RATE)\n","\n","model.summary()"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: SyntaxWarning: If f_max is given, n_bins will be ignored\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:137: SyntaxWarning: Using TPU, changing to compatible version\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/util.py:1361: NameBasedSaverStatus.__init__ (from tensorflow.python.training.tracking.util) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/util.py:1361: NameBasedSaverStatus.__init__ (from tensorflow.python.training.tracking.util) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input (InputLayer)              [(None, 4096, 3)]    0                                            \n","__________________________________________________________________________________________________\n","window (WindowingLayer)         (None, 4096, 3)      4096        input[0][0]                      \n","__________________________________________________________________________________________________\n","bandpass (BandpassLayer)        (None, 4096, 3)      2049        window[0][0]                     \n","__________________________________________________________________________________________________\n","cqt (CQTLayer)                  (None, 56, 64, 3)    229378      bandpass[0][0]                   \n","__________________________________________________________________________________________________\n","resize (Resizing)               (None, 260, 260, 3)  0           cqt[0][0]                        \n","__________________________________________________________________________________________________\n","permute (PermuteChannel)        (None, 260, 260, 3)  0           resize[0][0]                     \n","__________________________________________________________________________________________________\n","mask_t (TimeMask)               (None, 260, 260, 3)  0           permute[0][0]                    \n","                                                                 mask_t[0][0]                     \n","__________________________________________________________________________________________________\n","mask_f (FreqMask)               (None, 260, 260, 3)  0           mask_t[1][0]                     \n","                                                                 mask_f[0][0]                     \n","__________________________________________________________________________________________________\n","efficientnetv2-b2 (EffNetV2Mode (None, 1408)         8769374     mask_f[1][0]                     \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 1408)         0           efficientnetv2-b2[1][0]          \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 1)            1409        flatten[0][0]                    \n","==================================================================================================\n","Total params: 9,006,306\n","Trainable params: 8,690,544\n","Non-trainable params: 315,762\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"CNODhIFSuUdg","executionInfo":{"status":"ok","timestamp":1634919657128,"user_tz":-120,"elapsed":8,"user":{"displayName":"Salvador Belenguer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW_AZsUNzJIJvDzMeCJ9BrE28PuN1TO74vl78OUA=s64","userId":"08556756602404025388"}}},"source":["if Config.MODEL_PRELOAD:\n","    pretrained_model = Config.MODEL_PATH.joinpath(Config.MODEL_PRELOAD_NAME)\n","    local = tf.train.CheckpointOptions(experimental_io_device = \"/job:localhost\")\n","    if tf.io.gfile.isdir(pretrained_model):\n","        pretrained_model = tf.train.latest_checkpoint(pretrained_model)\n","    model.load_weights(pretrained_model, options = local)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5uqxf-CRtOI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e0fac560-2545-4f95-9406-de3e3ee15907"},"source":["# Train model with training and validation sets with checkpoints and control \n","# over training validation loss plateaus\n","if Config.MODEL_TRAIN:\n","    lr_callback = tf.keras.callbacks.ReduceLROnPlateau(patience = 1, monitor = \"val_loss\", cooldown = 0, verbose = 1)\n","    \n","    local = tf.train.CheckpointOptions(experimental_io_device='/job:localhost')\n","    check_callback = tf.keras.callbacks.ModelCheckpoint(\n","            filepath = Config.CKPT_PATH.joinpath(\"ckpt-{epoch:d}\"),\n","            save_weights_only = True,\n","            monitor = \"val_auc\",\n","            mode = \"max\",\n","            save_best_only = False,\n","            options = local)\n","    \n","    train_history = model.fit(training_ds, epochs = Config.EPOCHS, batch_size = Config.BATCH_SIZE, \n","                              validation_data = validation_ds, steps_per_epoch = spe_training,\n","                              validation_steps = spe_validation, callbacks = [lr_callback, check_callback])\n","\n","    #best_model_path = tf.train.latest_checkpoint(Config.CKPT_PATH)\n","    #model.load_weights(best_model_path, options = local)\n","\n","    Config.MODEL_PATH.mkdir(parents = True, exist_ok = True)\n","    model.save_weights(Config.MODEL_PATH.joinpath(Config.MODEL_SAVE_NAME), options = local)\n","\n","    train_hist_df = pd.DataFrame(train_history.history)\n","    train_hist_df.to_csv(Config.HISTORY_NAME, index = False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1425/4288 [========>.....................] - ETA: 7:09 - loss: 0.4939 - auc: 0.8137"]}]},{"cell_type":"markdown","metadata":{"id":"CQGL2yztxsys"},"source":["### 3.5 Predicting with the Model"]},{"cell_type":"markdown","metadata":{"id":"288sVKRlx8GV"},"source":["Once you are happy with the model, it is time to make predictions on the test set and explore a bit what is actually doing. First, try to obtain the predictions for the full test dataset and plot the output distribution."]},{"cell_type":"code","metadata":{"id":"ckvqjsTlRtOI"},"source":["# Predict on test set and save to submission file\n","if Config.MODEL_PREDICT:\n","    preds_test = model.predict(test_ds, batch_size = Config.BATCH_SIZE_TEST, \n","                               steps = spe_test, verbose = 1)\n","\n","    sub_df = pd.DataFrame({\n","        \"id\": test_ids,\n","        \"target\": preds_test.flatten()\n","    })\n","\n","    sub_df = sub_df.sort_values(\"id\").reset_index(drop = True)\n","        \n","    sub_df.to_csv(Config.PREDICTIONS_NAME, index = False)\n","\n","    plt.style.use(\"seaborn\")\n","    print()\n","    print(\"Test dataset output distribution\")\n","    sns.displot(sub_df, x = \"target\", kind = \"kde\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"azXTbT9QzdYI"},"source":["Now let's try to break down the model into its constituent parts and plot the intermediate data for a single example. You might see that, depending on the example, visually identifying a merger chirp becomes difficult or even impossible mainly due other low and high frequency sources of noise (in this case simulated)."]},{"cell_type":"code","metadata":{"id":"7v4z4mOmzd5M"},"source":["if Config.PLOT_EXAMPLE:\n","\n","    permute = model.get_layer(\"permute\")\n","    window = model.get_layer(\"window\")\n","    bandpass = model.get_layer(\"bandpass\")\n","    cqt = model.get_layer(\"cqt\")\n","    resize = model.get_layer(\"resize\")\n","    permute = model.get_layer(\"permute\")\n","    mask_t = model.get_layer(\"mask_t\")\n","    mask_f = model.get_layer(\"mask_f\")\n","    effnet = model.get_layer(Config.MODEL_ID)\n","    flatten = model.get_layer(\"flatten\")\n","    dense = model.get_layer(\"dense\")\n","        \n","    if Config.PLOT_TEST:\n","        for data, _ in test_ds.take(1):\n","            x_ref = data.numpy()[0, ...][np.newaxis, ...]\n","    else:\n","        for data, _ in training_ds.take(1):\n","            x_ref = data.numpy()[0, ...][np.newaxis, ...]\n","\n","    x = np.squeeze(x_ref)\n","    y = x_ref\n","        \n","    y = window(y)\n","    y_win = np.squeeze(y.numpy())\n","    y = bandpass(y)\n","    y_band = np.squeeze(y.numpy())\n","    y = cqt(y, training = False)\n","    y = resize(y)\n","    y_spec = np.squeeze(y.numpy())\n","    y = permute(y, training = True)\n","\n","    for _ in range(Config.N_MAX_MASK):\n","        y = mask_t(y, training = True)\n","    for _ in range(Config.N_MAX_MASK):\n","        y = mask_f(y, training = True)\n","\n","    y_masked = np.squeeze(y.numpy())\n","    y = effnet(y)\n","    y = flatten(y)\n","    y = dense(y)\n","    y_dense = np.squeeze(y.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hThBqe32wYDM"},"source":["if Config.PLOT_EXAMPLE:\n","    print(\"Standardised signals\")\n","    PlottingUtilities.plot_wave(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q7Hk5NAIwnnL"},"source":["if Config.PLOT_EXAMPLE:\n","    print(\"Tappered signals\")\n","    PlottingUtilities.plot_wave(y_win)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NYzTBF8Twrfb"},"source":["if Config.PLOT_EXAMPLE:\n","    print(\"Filtered signals\")\n","    PlottingUtilities.plot_wave(y_band)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8zptaWqwuNa"},"source":["if Config.PLOT_EXAMPLE:\n","    print(\"CQT spectrogram\")\n","    PlottingUtilities.plot_spectrogram(y_spec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHDuWwIawwsW"},"source":["if Config.PLOT_EXAMPLE:\n","    print(\"Masked CQT spectrogram\")\n","    PlottingUtilities.plot_spectrogram(y_masked)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MvjyxodczQSz"},"source":["Don't forget to save the results to your Google Drive! Make sure you have enough space as some models are truly heavy."]},{"cell_type":"code","metadata":{"id":"XuBWafX16UUy"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\", force_remount = True)\n","os.environ[\"OUT_NAME\"] = \"g2net_output\"\n","!echo $OUT_NAME\n","!mkdir -p $OUT_NAME\n","!cp -r checkpoints $OUT_NAME\n","!cp -r models $OUT_NAME\n","!cp -r *.csv $OUT_NAME\n","!cp -r $OUT_NAME /content/gdrive/MyDrive/\n","drive.flush_and_unmount()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VzTLh7qt9YwI"},"source":["\n","The best G2Net single model was obtained with trainable Tukey window and bandpass layers, a non-trainable CQT, a resize size of 384, an EfficientNet v2 S backbone and training in a Tesla V100 (16GB) GPU with a batch size of 32 (learning rates in the range 0.0003 to 0.00001. The score was boosted with an averaging ensemble of several models with different settings. \n","\n","The ROC & Roll team encourages you to play with the code, propose improvements and even introduce new layers that might be of use!"]}]}